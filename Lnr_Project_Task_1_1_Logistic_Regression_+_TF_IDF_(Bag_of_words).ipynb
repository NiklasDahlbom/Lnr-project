{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRaFjNBjkWLP"
      },
      "source": [
        "### Lnr Project Task 1.1 #  Logistic Regression + TF-IDF (Bag of words)\n",
        "\n",
        "Niklas Dahlbom, ndahlbom@kth.se, ndahlbo@upv.edu.es"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgxurtiflY0S"
      },
      "source": [
        "###Installs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "A3Bnd2x3jmAR",
        "outputId": "3f07d634-aa6e-43a2-d313-709d15edc3b9"
      },
      "outputs": [],
      "source": [
        "!pip install transformers --upgrade\n",
        "!pip install datasets accelerate --upgrade\n",
        "!pip install peft --upgrade\n",
        "!pip install jupyter --upgrade\n",
        "!pip install ipywidgets --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d1Owhk0xnFc"
      },
      "source": [
        "###Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bCiZ3-fmlStK"
      },
      "outputs": [],
      "source": [
        "from readerEXIST2025 import EXISTReader\n",
        "import re\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8a0qOw1yX9G"
      },
      "source": [
        "### Read datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQB83Hpvlzco",
        "outputId": "5e07082f-8a8f-4d0e-ce45-873e5a30c6a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1       Writing a uni essay in my local pub with a cof...\n",
            "2       @UniversalORL it is 2021 not 1921. I dont appr...\n",
            "5       According to a customer I have plenty of time ...\n",
            "6       So only 'blokes' drink beer? Sorry, but if you...\n",
            "7       New to the shelves this week - looking forward...\n",
            "                              ...                        \n",
            "3255    idk why y’all bitches think having half your a...\n",
            "3256    This has been a part of an experiment with @Wo...\n",
            "3257    \"Take me already\" \"Not yet. You gotta be ready...\n",
            "3258    @clintneedcoffee why do you look like a whore?...\n",
            "3259    ik when mandy says “you look like a whore” i l...\n",
            "Name: text, Length: 2870, dtype: object\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "reader_train = EXISTReader(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/training/EXIST2025_training.json\")\n",
        "reader_dev = EXISTReader(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/dev/EXIST2025_dev.json\")\n",
        "reader_test = EXISTReader(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/test/EXIST2025_test_clean.json\")\n",
        "\n",
        "EnTrainTask1, EnDevTask1 = reader_train.get(lang=\"EN\", subtask=\"1\"), reader_dev.get(lang=\"EN\", subtask=\"1\")\n",
        "SpTrainTask1, SpDevTask1 = reader_train.get(lang=\"ES\", subtask=\"1\"), reader_dev.get(lang=\"ES\", subtask=\"1\")\n",
        "\n",
        "SpTestTask1, EnTestTask1 = reader_test.get(lang=\"ES\", subtask=\"1\", include_ambiguous=True),  reader_test.get(lang=\"EN\", subtask=\"1\", include_ambiguous=True)\n",
        "\n",
        "print(EnTrainTask1[1])\n",
        "print(\"-------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JMr4Dr8yWWN",
        "outputId": "5af357c7-4c59-4dbf-8754-07eec8210260"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NO: 1733\n",
            "YES: 1137\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "label_counts = Counter(EnTrainTask1[2])\n",
        "print(\"NO:\", label_counts[\"NO\"])\n",
        "print(\"YES:\", label_counts[\"YES\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwpywTpSytMt"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zJ6VoJw4yu4_"
      },
      "outputs": [],
      "source": [
        "def clean_text(text_list):\n",
        "    cleaned_corpus = []\n",
        "    for text in text_list:\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"https?://\\S+\", \"\", text)  # Removes URLs\n",
        "        text = re.sub(r\"@\\w+\", \"\", text)          # Removes mentions\n",
        "        text = text.replace(\"#\", \"\")              # Removes Hashtags\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()   # Removes spaces\n",
        "        cleaned_corpus.append(text)\n",
        "    return cleaned_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dp_oaw2Rxed"
      },
      "source": [
        "### Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54Skd0eXWZWq"
      },
      "source": [
        "### Baseline: Logistic Regression + TF-IDF (Bag of words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_qxgo8SqX-cV",
        "outputId": "260190fb-972c-473f-a94f-ec7d3b7d1ffc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scikit-learn in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (2.2.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hTXlQiVx7wVr"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(y_true, y_pred):\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='binary', zero_division=0, pos_label=1\n",
        "    )\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "\n",
        "    print(f\"Accuracy: {acc:.3f}\")\n",
        "    print(f\"Binary F1: {f1:.3f}\")\n",
        "    print(f\"Precision: {precision:.3f}\")\n",
        "    print(f\"Recall: {recall:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1k5tGoCdRy8L",
        "outputId": "a9ef284f-56a1-46ba-eb7d-7358eef95611"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.768\n",
            "Binary F1: 0.645\n",
            "Precision: 0.786\n",
            "Recall: 0.548\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
            "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
            "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
            "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Example: assuming `tweets` = list of tweet texts, and `labels` = list of YES/NO\n",
        "tweets = EnTrainTask1[1]\n",
        "labels = EnTrainTask1[2]\n",
        "\n",
        "tweets_clean = clean_text(tweets)  # Your cleaning function here\n",
        "\n",
        "# Convert YES/NO to 1/0\n",
        "labels_bin = [1 if l == \"YES\" else 0 for l in labels]\n",
        "\n",
        "# Train/val split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    tweets_clean, labels_bin, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# TF-IDF + Logistic Regression pipeline\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Evaluation\n",
        "y_pred = clf.predict(X_val_tfidf)\n",
        "\n",
        "# Compute and print custom metrics (main focus)\n",
        "compute_metrics(y_val, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save to cvs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved predictions to BoW_predictions_task1.csv ✅\n"
          ]
        }
      ],
      "source": [
        "# 1. Prepare and clean the test data\n",
        "test_ids = EnTestTask1[0]\n",
        "test_texts = EnTestTask1[1]\n",
        "test_clean = clean_text(test_texts)  # Reuse your cleaning function\n",
        "\n",
        "# 2. Transform test data using trained vectorizer\n",
        "X_test_tfidf = vectorizer.transform(test_clean)\n",
        "\n",
        "# 3. Predict test labels\n",
        "test_preds_bin = clf.predict(X_test_tfidf)\n",
        "\n",
        "# 4. Convert binary predictions back to YES/NO\n",
        "test_preds_label = [\"YES\" if p == 1 else \"NO\" for p in test_preds_bin]\n",
        "\n",
        "# 5. Create DataFrame for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    \"id\": test_ids,\n",
        "    \"label\": test_preds_label,\n",
        "    \"test_case\": [\"EXIST2025\"] * len(test_ids)\n",
        "})\n",
        "\n",
        "# 6. Save to CSV\n",
        "submission_df.to_csv(\"BoW_predictions_task1.csv\", index=False)\n",
        "print(\"Saved predictions to BoW_predictions_task1.csv ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save to json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to task1_submission.json ✅\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Load your CSV results\n",
        "df = pd.read_csv(\"BoW_predictions_task1.csv\")\n",
        "\n",
        "# Create a list of dictionaries in the required format\n",
        "results_json = []\n",
        "for _, row in df.iterrows():\n",
        "    result = {\n",
        "        \"id\": f\"{row['id']}\",  # add prefix 'id_' as required\n",
        "        \"value\": row[\"label\"],    # if you only have hard outputs (YES/NO)\n",
        "        \"test_case\": row[\"test_case\"]\n",
        "    }\n",
        "    results_json.append(result)\n",
        "\n",
        "# Save to JSON file\n",
        "with open(\"BoW_task1_submission.json\", \"w\") as f:\n",
        "    json.dump(results_json, f, indent=2)\n",
        "\n",
        "print(\"Saved to task1_submission.json ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spanish"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.740\n",
            "Binary F1: 0.710\n",
            "Precision: 0.738\n",
            "Recall: 0.684\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
            "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
            "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
            "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n"
          ]
        }
      ],
      "source": [
        "# Example: assuming `tweets` = list of tweet texts, and `labels` = list of YES/NO\n",
        "tweets = SpTrainTask1[1]\n",
        "labels = SpTrainTask1[2]\n",
        "\n",
        "tweets_clean = clean_text(tweets)  # Your cleaning function here\n",
        "\n",
        "# Convert YES/NO to 1/0\n",
        "labels_bin = [1 if l == \"YES\" else 0 for l in labels]\n",
        "\n",
        "# Train/val split\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    tweets_clean, labels_bin, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# TF-IDF + Logistic Regression pipeline\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_val_tfidf = vectorizer.transform(X_val)\n",
        "\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Evaluation\n",
        "y_pred = clf.predict(X_val_tfidf)\n",
        "\n",
        "# Compute and print custom metrics (main focus)\n",
        "compute_metrics(y_val, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save to csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved predictions to BoW_predictions_task1_Spanish.csv ✅\n"
          ]
        }
      ],
      "source": [
        "# 1. Prepare and clean the test data\n",
        "test_ids = SpTestTask1[0]\n",
        "test_texts = SpTestTask1[1]\n",
        "test_clean = clean_text(test_texts)  # Reuse your cleaning function\n",
        "\n",
        "# 2. Transform test data using trained vectorizer\n",
        "X_test_tfidf = vectorizer.transform(test_clean)\n",
        "\n",
        "# 3. Predict test labels\n",
        "test_preds_bin = clf.predict(X_test_tfidf)\n",
        "\n",
        "# 4. Convert binary predictions back to YES/NO\n",
        "test_preds_label = [\"YES\" if p == 1 else \"NO\" for p in test_preds_bin]\n",
        "\n",
        "# 5. Create DataFrame for submission\n",
        "submission_df = pd.DataFrame({\n",
        "    \"id\": test_ids,\n",
        "    \"label\": test_preds_label,\n",
        "    \"test_case\": [\"EXIST2025\"] * len(test_ids)\n",
        "})\n",
        "\n",
        "# 6. Save to CSV\n",
        "submission_df.to_csv(\"BoW_predictions_task1_Spanish.csv\", index=False)\n",
        "print(\"Saved predictions to BoW_predictions_task1_Spanish.csv ✅\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save to json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved to task1_submission_Spanish.json ✅\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Load your CSV results\n",
        "df = pd.read_csv(\"BoW_predictions_task1_Spanish.csv\")\n",
        "\n",
        "# Create a list of dictionaries in the required format\n",
        "results_json = []\n",
        "for _, row in df.iterrows():\n",
        "    result = {\n",
        "        \"id\": f\"{row['id']}\",  # add prefix 'id_' as required\n",
        "        \"value\": row[\"label\"],    # if you only have hard outputs (YES/NO)\n",
        "        \"test_case\": row[\"test_case\"]\n",
        "    }\n",
        "    results_json.append(result)\n",
        "\n",
        "# Save to JSON file\n",
        "with open(\"BoW_task1_submission_Spanish.json\", \"w\") as f:\n",
        "    json.dump(results_json, f, indent=2)\n",
        "\n",
        "print(\"Saved to task1_submission_Spanish.json ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Merge results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged 978 EN + 1098 ES = 2076 total predictions.\n"
          ]
        }
      ],
      "source": [
        "filename_english = \"BoW_task1_submission.json\"\n",
        "filename_spanish = \"BoW_task1_submission_Spanish.json\"\n",
        "filename_merged = \"BoW_task1_submission_merged.json\"\n",
        "\n",
        "def merge_predictions(filename_english, filename_spanish, filename_merged):\n",
        "    # Load English predictions\n",
        "    with open(filename_english, \"r\", encoding=\"utf-8\") as f_en:\n",
        "        preds_en = json.load(f_en)\n",
        "\n",
        "    # Load Spanish predictions\n",
        "    with open(filename_spanish, \"r\", encoding=\"utf-8\") as f_es:\n",
        "        preds_es = json.load(f_es)\n",
        "\n",
        "    # Merge the two lists\n",
        "    merged_preds = preds_es + preds_en\n",
        "\n",
        "    # Save the combined predictions\n",
        "    with open(filename_merged, \"w\", encoding=\"utf-8\") as f_out:\n",
        "        json.dump(merged_preds, f_out, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Merged {len(preds_en)} EN + {len(preds_es)} ES = {len(merged_preds)} total predictions.\")\n",
        "    \n",
        "    \n",
        "merge_predictions(\n",
        "    filename_english,\n",
        "    filename_spanish,\n",
        "    filename_merged\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
