{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb3010d",
   "metadata": {},
   "source": [
    "### Lnr Project Task 1.2 #  Logistic Regression + TF-IDF (Bag of words)\n",
    "\n",
    "Niklas Dahlbom, ndahlbom@kth.se, ndahlbo@upv.edu.es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394d9fa2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f2629d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from readerEXIST2025 import EXISTReader\n",
    "import re\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a64b96",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5ec88115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1       Writing a uni essay in my local pub with a cof...\n",
      "2       @UniversalORL it is 2021 not 1921. I dont appr...\n",
      "5       According to a customer I have plenty of time ...\n",
      "6       So only 'blokes' drink beer? Sorry, but if you...\n",
      "10      #EverydaySexism means women usually end up in ...\n",
      "                              ...                        \n",
      "3253    @ShefVaidya Ma'am if I say that you look like ...\n",
      "3255    idk why y’all bitches think having half your a...\n",
      "3256    This has been a part of an experiment with @Wo...\n",
      "3257    \"Take me already\" \"Not yet. You gotta be ready...\n",
      "3258    @clintneedcoffee why do you look like a whore?...\n",
      "Name: text, Length: 856, dtype: object\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "reader_train = EXISTReader(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/training/EXIST2025_training.json\")\n",
    "reader_dev = EXISTReader(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/dev/EXIST2025_dev.json\")\n",
    "reader_test = EXISTReader(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/test/EXIST2025_test_clean.json\")\n",
    "\n",
    "EnTrainTask2, EnDevTask2 = reader_train.get(lang=\"EN\", subtask=\"2\"), reader_dev.get(lang=\"EN\", subtask=\"2\")\n",
    "SpTrainTask2, SpDevTask2 = reader_train.get(lang=\"ES\", subtask=\"2\"), reader_dev.get(lang=\"ES\", subtask=\"2\")\n",
    "\n",
    "SpTestTask2, EnTestTask2 = reader_test.get(lang=\"ES\", subtask=\"2\", include_ambiguous=True),  reader_test.get(lang=\"EN\", subtask=\"2\", include_ambiguous=True)\n",
    "\n",
    "print(EnTrainTask2[1])\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36086650",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e249fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_list):\n",
    "    cleaned_corpus = []\n",
    "    for text in text_list:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"https?://\\S+\", \"\", text)  # Removes URLs\n",
    "        text = re.sub(r\"@\\w+\", \"\", text)          # Removes mentions\n",
    "        text = text.replace(\"#\", \"\")              # Removes Hashtags\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()   # Removes spaces\n",
    "        cleaned_corpus.append(text)\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "08c2bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {acc:.3f}\")\n",
    "    print(f\"Macro F1: {f1:.3f}\")\n",
    "    print(f\"Macro Precision: {precision:.3f}\")\n",
    "    print(f\"Macro Recall: {recall:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bbf769a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_to_json(yes_ids, yes_labels, output_path, lang=\"en\"):\n",
    "    import json\n",
    "\n",
    "    # Ensure yes_ids are strings\n",
    "    yes_ids = [str(id_).replace(\"id_\", \"\") for id_ in yes_ids]\n",
    "\n",
    "    # Load test data\n",
    "    with open(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/test/EXIST2025_test_clean.json\", \"r\") as f:\n",
    "        test_json = json.load(f)\n",
    "\n",
    "    # Get all IDs for the selected language\n",
    "    all_ids = [\n",
    "        str(tweet_data[\"id_EXIST\"])\n",
    "        for tweet_data in test_json.values()\n",
    "        if tweet_data[\"lang\"] == lang\n",
    "    ]\n",
    "\n",
    "    # Determine NO IDs\n",
    "    yes_ids_set = set(yes_ids)\n",
    "    all_ids_set = set(all_ids)\n",
    "    no_ids = list(all_ids_set - yes_ids_set)\n",
    "\n",
    "    # Build output JSON\n",
    "    output_data = []\n",
    "\n",
    "    # Add YES predictions\n",
    "    for tweet_id, label in zip(yes_ids, yes_labels):\n",
    "        output_data.append({\n",
    "            \"id\": tweet_id,\n",
    "            \"value\": label,\n",
    "            \"test_case\": \"EXIST2025\"\n",
    "        })\n",
    "\n",
    "    # Add NO predictions\n",
    "    for tweet_id in no_ids:\n",
    "        output_data.append({\n",
    "            \"id\": tweet_id,\n",
    "            \"value\": \"NO\",\n",
    "            \"test_case\": \"EXIST2025\"\n",
    "        })\n",
    "\n",
    "    # Sort by ID numerically\n",
    "    output_data.sort(key=lambda x: int(x[\"id\"]))\n",
    "\n",
    "    # Save to file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(output_data)} predictions to {output_path} (YES: {len(yes_ids)}, NO: {len(no_ids)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2dc07d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Extract training and validation data ===\n",
    "training_tweets_raw = EnTrainTask2[1]      # Raw tweet texts from the training set\n",
    "training_labels_raw = EnTrainTask2[2]      # Corresponding labels for training tweets\n",
    "\n",
    "validation_tweets_raw = EnDevTask2[1]      # Raw tweet texts from the validation set\n",
    "validation_labels_raw = EnDevTask2[2]      # Corresponding labels for validation tweets\n",
    "\n",
    "# === 3. Clean the tweets (e.g., remove URLs, mentions, special characters) ===\n",
    "cleaned_training_tweets = clean_text(training_tweets_raw)\n",
    "cleaned_validation_tweets = clean_text(validation_tweets_raw)\n",
    "\n",
    "# === 4. Encode string labels as numeric values for model training ===\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_training_labels = label_encoder.fit_transform(training_labels_raw)\n",
    "encoded_validation_labels = label_encoder.transform(validation_labels_raw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94893496",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f780e6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.623\n",
      "Macro F1: 0.536\n",
      "Macro Precision: 0.543\n",
      "Macro Recall: 0.540\n",
      "Note: This evaluation is performed on the full validation set, not just tweets predicted as YES by previous models.\n"
     ]
    }
   ],
   "source": [
    "# === 5. Convert cleaned text data into TF-IDF features ===\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))  # Use unigrams and bigrams\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(cleaned_training_tweets)    # Learn vocab and transform train data\n",
    "X_val_tfidf = tfidf_vectorizer.transform(cleaned_validation_tweets)        # Transform validation data\n",
    "\n",
    "# === 6. Train a Logistic Regression classifier ===\n",
    "logistic_regressor = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "logistic_regressor.fit(X_train_tfidf, encoded_training_labels)\n",
    "\n",
    "# === 7. Predict and evaluate on the validation set ===\n",
    "validation_predictions = logistic_regressor.predict(X_val_tfidf)\n",
    "\n",
    "# Calculate and display custom evaluation metrics\n",
    "compute_metrics(encoded_validation_labels, validation_predictions)\n",
    "\n",
    "print(\"Note: This evaluation is performed on the full validation set, not just tweets predicted as YES by previous models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "eabd0922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class DIRECT: 535 samples\n",
      "Class JUDGEMENTAL: 137 samples\n",
      "Class REPORTED: 184 samples\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(encoded_training_labels, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Class {label_encoder.inverse_transform([u])[0]}: {c} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a39cfc",
   "metadata": {},
   "source": [
    "### Get previous YES statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2f1a0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(ids):\n",
    "    with open(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/test/EXIST2025_test_clean.json\", \"r\") as f:\n",
    "        test_json = json.load(f)\n",
    "    # Filter to only English tweets in test set (optional sanity check)\n",
    "    tweets_all = [item for item in test_json.values() if item[\"lang\"] == \"en\"]\n",
    "\n",
    "    print(f\"Total English tweets in test set: {len(tweets_all)}\")\n",
    "\n",
    "    # Filter only English test tweets where id_EXIST is in yes_ids\n",
    "    yes_tweets = [\n",
    "        tweet_data[\"tweet\"]\n",
    "        for tweet_data in test_json.values()\n",
    "        if tweet_data[\"lang\"] == \"en\" and tweet_data[\"id_EXIST\"] in ids\n",
    "    ]\n",
    "\n",
    "    yes_en_ids = [\n",
    "        tweet_data[\"id_EXIST\"]\n",
    "        for tweet_data in test_json.values()\n",
    "        if tweet_data[\"lang\"] == \"en\" and tweet_data[\"id_EXIST\"] in ids\n",
    "    ]\n",
    "\n",
    "    print(f\"Number of English tweets predicted YES: {len(yes_tweets)}\")\n",
    "    print(yes_en_ids[:5])\n",
    "    print(yes_tweets[:5])\n",
    "    \n",
    "    return yes_en_ids, yes_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23398e38",
   "metadata": {},
   "source": [
    "### BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c12f3838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['600002', '600013', '600014', '600019', '600027']\n"
     ]
    }
   ],
   "source": [
    "# Load your BoW predictions from the JSON file\n",
    "with open(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/Task 1.1/BoW/BoW_task1_submission.json\", \"r\") as f:\n",
    "    bow_results = json.load(f)\n",
    "\n",
    "# Extract the IDs that were predicted as \"YES\"\n",
    "yes_ids_bow = [entry[\"id\"] for entry in bow_results if entry[\"value\"] == \"YES\"]\n",
    "yes_ids_bow = [id_.replace(\"id_\", \"\") for id_ in yes_ids_bow]\n",
    "\n",
    "\n",
    "print(yes_ids_bow[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3671ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ids, filtered_tweets = get_ids(yes_ids_bow)\n",
    "\n",
    "cleaned_filtered_tweets = clean_text(filtered_tweets)\n",
    "X_filtered_tfidf = tfidf_vectorizer.transform(cleaned_filtered_tweets)\n",
    "\n",
    "filtered_predictions = logistic_regressor.predict(X_filtered_tfidf)\n",
    "\n",
    "filtered_pred_labels = label_encoder.inverse_transform(filtered_predictions)\n",
    "\n",
    "for tweet, label in zip(filtered_tweets[:5], filtered_pred_labels[:5]):\n",
    "    print(f\"Tweet: {tweet}\\nPredicted label: {label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a05d8",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "942b8169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['600002', '600005', '600013', '600019', '600020']\n"
     ]
    }
   ],
   "source": [
    "# Load your BoW predictions from the JSON file\n",
    "with open(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/Task 1.1/XGBoost/xgb_task1_submission.json\", \"r\") as f:\n",
    "    xgb_results = json.load(f)\n",
    "\n",
    "# Extract the IDs that were predicted as \"YES\"\n",
    "yes_ids_xgb = [entry[\"id\"] for entry in xgb_results if entry[\"value\"] == \"YES\"]\n",
    "yes_ids_xgb = [id_.replace(\"id_\", \"\") for id_ in yes_ids_xgb]\n",
    "\n",
    "\n",
    "print(yes_ids_xgb[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10db7bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ids, filtered_tweets = get_ids(yes_ids_xgb)\n",
    "\n",
    "cleaned_filtered_tweets = clean_text(filtered_tweets)\n",
    "X_filtered_tfidf = tfidf_vectorizer.transform(cleaned_filtered_tweets)\n",
    "\n",
    "filtered_predictions = logistic_regressor.predict(X_filtered_tfidf)\n",
    "\n",
    "filtered_pred_labels = label_encoder.inverse_transform(filtered_predictions)\n",
    "\n",
    "for tweet, label in zip(filtered_tweets[:5], filtered_pred_labels[:5]):\n",
    "    print(f\"Tweet: {tweet}\\nPredicted label: {label}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30da94a",
   "metadata": {},
   "source": [
    "### Bert (Best perfomance in tast 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c735d207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['600002', '600004', '600005', '600013', '600018']\n"
     ]
    }
   ],
   "source": [
    "# Load your BoW predictions from the JSON file\n",
    "with open(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/Task 1.1/Bert/bert_task1_submission.json\", \"r\") as f:\n",
    "    bert_results = json.load(f)\n",
    "\n",
    "# Extract the IDs that were predicted as \"YES\"\n",
    "yes_ids_bert = [entry[\"id\"] for entry in bert_results if entry[\"value\"] == \"YES\"]\n",
    "yes_ids_bert = [id_.replace(\"id_\", \"\") for id_ in yes_ids_bert]\n",
    "\n",
    "\n",
    "print(yes_ids_bert[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "44abfd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total English tweets in test set: 978\n",
      "Number of English tweets predicted YES: 460\n",
      "['600002', '600004', '600005', '600013', '600018']\n",
      "[\"@Cathy_TwoHalves @EverydaySexism That's why women have to stay at home, raise children and take care of the house.Nobody is going to assume that a woman is a leader among men since that's not how it is supposed to be in nature. Of course, you can try and fake it, but you will keep losing your feminine side.\", '@JonPincince @kmulvane @EverydaySexism, even after Roe v. Wade was overturned. Fun! /s', 'Seriously @JohnLewisRetail?  girls are pictured with ‘I’m a Stylist’ doll and the pizza oven - the boys get a robot and hot wheels garage. #EverydaySexism https://t.co/mKsbxaGgIV? # via @HPUKParents', '@tired_of_debate @ScoutSleepe @ImWatson91 The fact that some men assault transwomen too is awful &amp; unacceptable.Having said that—being harassed by men does NOT render MtFs “women” (+men *do* harass other men sometimes, too…)+It is notable that—for ALL sexual harassment/assault victims—the abuser is almost always MALE!', 'it’s not even 7pm and we are already being harassed by a drunk girl on the blue line and bonding with a fellow rider about it lmao']\n",
      "Tweet: @Cathy_TwoHalves @EverydaySexism That's why women have to stay at home, raise children and take care of the house.Nobody is going to assume that a woman is a leader among men since that's not how it is supposed to be in nature. Of course, you can try and fake it, but you will keep losing your feminine side.\n",
      "Predicted label: JUDGEMENTAL\n",
      "\n",
      "Tweet: @JonPincince @kmulvane @EverydaySexism, even after Roe v. Wade was overturned. Fun! /s\n",
      "Predicted label: DIRECT\n",
      "\n",
      "Tweet: Seriously @JohnLewisRetail?  girls are pictured with ‘I’m a Stylist’ doll and the pizza oven - the boys get a robot and hot wheels garage. #EverydaySexism https://t.co/mKsbxaGgIV? # via @HPUKParents\n",
      "Predicted label: REPORTED\n",
      "\n",
      "Tweet: @tired_of_debate @ScoutSleepe @ImWatson91 The fact that some men assault transwomen too is awful &amp; unacceptable.Having said that—being harassed by men does NOT render MtFs “women” (+men *do* harass other men sometimes, too…)+It is notable that—for ALL sexual harassment/assault victims—the abuser is almost always MALE!\n",
      "Predicted label: REPORTED\n",
      "\n",
      "Tweet: it’s not even 7pm and we are already being harassed by a drunk girl on the blue line and bonding with a fellow rider about it lmao\n",
      "Predicted label: REPORTED\n",
      "\n",
      "Saved 978 predictions to BoW_task2_submission.json (YES: 460, NO: 518)\n"
     ]
    }
   ],
   "source": [
    "filtered_ids, filtered_tweets = get_ids(yes_ids_bert)\n",
    "\n",
    "cleaned_filtered_tweets = clean_text(filtered_tweets)\n",
    "X_filtered_tfidf = tfidf_vectorizer.transform(cleaned_filtered_tweets)\n",
    "\n",
    "filtered_predictions = logistic_regressor.predict(X_filtered_tfidf)\n",
    "\n",
    "filtered_pred_labels = label_encoder.inverse_transform(filtered_predictions)\n",
    "\n",
    "for tweet, label in zip(filtered_tweets[:5], filtered_pred_labels[:5]):\n",
    "    print(f\"Tweet: {tweet}\\nPredicted label: {label}\\n\")\n",
    "\n",
    "save_predictions_to_json(filtered_ids, filtered_pred_labels, \"BoW_task2_submission.json\", lang=\"en\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b9f30f",
   "metadata": {},
   "source": [
    "### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1698bbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2. Extract training and validation data ===\n",
    "training_tweets_raw = SpTrainTask2[1]      # Raw tweet texts from the training set\n",
    "training_labels_raw = SpTrainTask2[2]      # Corresponding labels for training tweets\n",
    "\n",
    "validation_tweets_raw = SpDevTask2[1]      # Raw tweet texts from the validation set\n",
    "validation_labels_raw = SpDevTask2[2]      # Corresponding labels for validation tweets\n",
    "\n",
    "# === 3. Clean the tweets (e.g., remove URLs, mentions, special characters) ===\n",
    "cleaned_training_tweets = clean_text(training_tweets_raw)\n",
    "cleaned_validation_tweets = clean_text(validation_tweets_raw)\n",
    "\n",
    "# === 4. Encode string labels as numeric values for model training ===\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_training_labels = label_encoder.fit_transform(training_labels_raw)\n",
    "encoded_validation_labels = label_encoder.transform(validation_labels_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "acb69eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.623\n",
      "Macro F1: 0.564\n",
      "Macro Precision: 0.561\n",
      "Macro Recall: 0.568\n",
      "Note: This evaluation is performed on the full validation set, not just tweets predicted as YES by previous models.\n"
     ]
    }
   ],
   "source": [
    "# === 5. Convert cleaned text data into TF-IDF features ===\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))  # Use unigrams and bigrams\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(cleaned_training_tweets)    # Learn vocab and transform train data\n",
    "X_val_tfidf = tfidf_vectorizer.transform(cleaned_validation_tweets)        # Transform validation data\n",
    "\n",
    "# === 6. Train a Logistic Regression classifier ===\n",
    "logistic_regressor = LogisticRegression(max_iter=1000, class_weight='balanced')\n",
    "logistic_regressor.fit(X_train_tfidf, encoded_training_labels)\n",
    "\n",
    "# === 7. Predict and evaluate on the validation set ===\n",
    "validation_predictions = logistic_regressor.predict(X_val_tfidf)\n",
    "\n",
    "# Calculate and display custom evaluation metrics\n",
    "compute_metrics(encoded_validation_labels, validation_predictions)\n",
    "\n",
    "print(\"Note: This evaluation is performed on the full validation set, not just tweets predicted as YES by previous models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "98491c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class DIRECT: 737 samples\n",
      "Class JUDGEMENTAL: 222 samples\n",
      "Class REPORTED: 258 samples\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(encoded_training_labels, return_counts=True)\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Class {label_encoder.inverse_transform([u])[0]}: {c} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b868c",
   "metadata": {},
   "source": [
    "### Bert (Best performance in task 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c1423b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids_spanish(ids):\n",
    "    with open(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/test/EXIST2025_test_clean.json\", \"r\") as f:\n",
    "        test_json = json.load(f)\n",
    "    # Filter to only English tweets in test set (optional sanity check)\n",
    "    tweets_all = [item for item in test_json.values() if item[\"lang\"] == \"en\"]\n",
    "\n",
    "    print(f\"Total English tweets in test set: {len(tweets_all)}\")\n",
    "\n",
    "    # Filter only English test tweets where id_EXIST is in yes_ids\n",
    "    yes_tweets = [\n",
    "        tweet_data[\"tweet\"]\n",
    "        for tweet_data in test_json.values()\n",
    "        if tweet_data[\"lang\"] == \"es\" and tweet_data[\"id_EXIST\"] in ids\n",
    "    ]\n",
    "\n",
    "    yes_en_ids = [\n",
    "        tweet_data[\"id_EXIST\"]\n",
    "        for tweet_data in test_json.values()\n",
    "        if tweet_data[\"lang\"] == \"es\" and tweet_data[\"id_EXIST\"] in ids\n",
    "    ]\n",
    "\n",
    "    print(f\"Number of English tweets predicted YES: {len(yes_tweets)}\")\n",
    "    print(yes_en_ids[:5])\n",
    "    print(yes_tweets[:5])\n",
    "    \n",
    "    return yes_en_ids, yes_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a9cd0f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['500004', '500012', '500019', '500020', '500022']\n"
     ]
    }
   ],
   "source": [
    "# Load your BoW predictions from the JSON file\n",
    "with open(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/Task 1.1/Bert/bert_task1_submission_Spanish.json\", \"r\") as f:\n",
    "    bert_results = json.load(f)\n",
    "\n",
    "# Extract the IDs that were predicted as \"YES\"\n",
    "yes_ids_bert = [entry[\"id\"] for entry in bert_results if entry[\"value\"] == \"YES\"]\n",
    "yes_ids_bert = [id_.replace(\"id_\", \"\") for id_ in yes_ids_bert]\n",
    "\n",
    "\n",
    "print(yes_ids_bert[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "133a93ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total English tweets in test set: 978\n",
      "Number of English tweets predicted YES: 567\n",
      "['500004', '500012', '500019', '500020', '500022']\n",
      "['@jordirico Primero fue internet, luego el gamergate, la manosfera y su misoginia extrema sin que las plataformas movieran un dedo, los incel-asesinatos, la extrema derecha capitalizando el odio, la mimetización de estas ideas con las de un debate respetable y finalmente aquí estamos', '@dimplerrylover lo se pero literalmente la chica estaba siendo harassed like como puede sonreir asi', '@Fistroman1 @ElioGatsby Sin pruebas??Qué fue en público.Qué lo vimos todos, qué me estás contando.Que no me mezcles churras con merinas.Qué de aquí a que me vengas con el metoo y el notallmen queda poquito y no me apetece acabar discutiendo de verdad, q lo veo venir.', '@EstefaniaVeloz ...Con el proceso legal, no para hacer arder a una persona (No sé ni quienes sean en este caso). Hay mujeres que son víctimas pero también hay corruptas, violentas y victimarias. El movimiento #MeToo ya cumplió su cometido ¿Qué sigue? ¿Cómo avanzar sin que sea un arma a capricho?', '@geeksterilia Desde la Olivia que anda trepadota en ladrillo por ser directora.El video de la llama a Shia está así de la wey toda condescendiente para MI floQue para esto según ella le pidió a Shia retirarse de la pelo porque pues metooY Andale que nada. La wey fingiendo https://t.co/t9zCKx1IDs']\n",
      "Tweet: @jordirico Primero fue internet, luego el gamergate, la manosfera y su misoginia extrema sin que las plataformas movieran un dedo, los incel-asesinatos, la extrema derecha capitalizando el odio, la mimetización de estas ideas con las de un debate respetable y finalmente aquí estamos\n",
      "Predicted label: JUDGEMENTAL\n",
      "\n",
      "Tweet: @dimplerrylover lo se pero literalmente la chica estaba siendo harassed like como puede sonreir asi\n",
      "Predicted label: DIRECT\n",
      "\n",
      "Tweet: @Fistroman1 @ElioGatsby Sin pruebas??Qué fue en público.Qué lo vimos todos, qué me estás contando.Que no me mezcles churras con merinas.Qué de aquí a que me vengas con el metoo y el notallmen queda poquito y no me apetece acabar discutiendo de verdad, q lo veo venir.\n",
      "Predicted label: REPORTED\n",
      "\n",
      "Tweet: @EstefaniaVeloz ...Con el proceso legal, no para hacer arder a una persona (No sé ni quienes sean en este caso). Hay mujeres que son víctimas pero también hay corruptas, violentas y victimarias. El movimiento #MeToo ya cumplió su cometido ¿Qué sigue? ¿Cómo avanzar sin que sea un arma a capricho?\n",
      "Predicted label: JUDGEMENTAL\n",
      "\n",
      "Tweet: @geeksterilia Desde la Olivia que anda trepadota en ladrillo por ser directora.El video de la llama a Shia está así de la wey toda condescendiente para MI floQue para esto según ella le pidió a Shia retirarse de la pelo porque pues metooY Andale que nada. La wey fingiendo https://t.co/t9zCKx1IDs\n",
      "Predicted label: DIRECT\n",
      "\n",
      "Saved 1098 predictions to BoW_task2_submission_Spanish.json (YES: 567, NO: 531)\n"
     ]
    }
   ],
   "source": [
    "filtered_ids, filtered_tweets = get_ids_spanish(yes_ids_bert)\n",
    "\n",
    "cleaned_filtered_tweets = clean_text(filtered_tweets)\n",
    "X_filtered_tfidf = tfidf_vectorizer.transform(cleaned_filtered_tweets)\n",
    "\n",
    "filtered_predictions = logistic_regressor.predict(X_filtered_tfidf)\n",
    "\n",
    "filtered_pred_labels = label_encoder.inverse_transform(filtered_predictions)\n",
    "\n",
    "for tweet, label in zip(filtered_tweets[:5], filtered_pred_labels[:5]):\n",
    "    print(f\"Tweet: {tweet}\\nPredicted label: {label}\\n\")\n",
    "\n",
    "save_predictions_to_json(filtered_ids, filtered_pred_labels, \"BoW_task2_submission_Spanish.json\", lang=\"es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3648dece",
   "metadata": {},
   "source": [
    "### Merge files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "6f3f61bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 978 EN + 1098 ES = 2076 total predictions.\n"
     ]
    }
   ],
   "source": [
    "filename_english = \"BoW_task2_submission.json\"\n",
    "filename_spanish = \"BoW_task2_submission_Spanish.json\"\n",
    "filename_merged = \"BoW_task2_submission_merge.json\"\n",
    "\n",
    "def merge_predictions(filename_english, filename_spanish, filename_merged):\n",
    "    # Load English predictions\n",
    "    with open(filename_english, \"r\", encoding=\"utf-8\") as f_en:\n",
    "        preds_en = json.load(f_en)\n",
    "\n",
    "    # Load Spanish predictions\n",
    "    with open(filename_spanish, \"r\", encoding=\"utf-8\") as f_es:\n",
    "        preds_es = json.load(f_es)\n",
    "\n",
    "    # Merge the two lists\n",
    "    merged_preds = preds_es + preds_en\n",
    "\n",
    "    # Save the combined predictions\n",
    "    with open(filename_merged, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(merged_preds, f_out, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Merged {len(preds_en)} EN + {len(preds_es)} ES = {len(merged_preds)} total predictions.\")\n",
    "    \n",
    "    \n",
    "merge_predictions(\n",
    "    filename_english,\n",
    "    filename_spanish,\n",
    "    filename_merged\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
