{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRaFjNBjkWLP"
      },
      "source": [
        "### Lnr Project Task 1.3 Bert\n",
        "\n",
        "Niklas Dahlbom, ndahlbom@kth.se, ndahlbo@upv.edu.es"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d1Owhk0xnFc"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bCiZ3-fmlStK",
        "outputId": "4821ec09-4f65-443e-aae9-212fa7dd7fcd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from readerEXIST2025 import EXISTReader\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import  AutoTokenizer, AutoModelForSequenceClassification,  Trainer, TrainingArguments,  EarlyStoppingCallback\n",
        "import tempfile\n",
        "from pyevall.evaluation import PyEvALLEvaluation\n",
        "from pyevall.metrics.metricfactory import MetricFactory\n",
        "from pyevall.reports.reports import PyEvALLReport\n",
        "from pyevall.utils.utils import PyEvALLUtils\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8a0qOw1yX9G"
      },
      "source": [
        "### Read datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQB83Hpvlzco",
        "outputId": "20049016-dd14-4aac-9181-2cf6bd58d78f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1       Writing a uni essay in my local pub with a cof...\n",
            "2       @UniversalORL it is 2021 not 1921. I dont appr...\n",
            "5       According to a customer I have plenty of time ...\n",
            "6       So only 'blokes' drink beer? Sorry, but if you...\n",
            "7       New to the shelves this week - looking forward...\n",
            "                              ...                        \n",
            "3255    idk why y’all bitches think having half your a...\n",
            "3256    This has been a part of an experiment with @Wo...\n",
            "3257    \"Take me already\" \"Not yet. You gotta be ready...\n",
            "3258    @clintneedcoffee why do you look like a whore?...\n",
            "3259    ik when mandy says “you look like a whore” i l...\n",
            "Name: text, Length: 2095, dtype: object\n",
            "-------------------\n"
          ]
        }
      ],
      "source": [
        "reader_train = EXISTReader(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/training/EXIST2025_training.json\")\n",
        "reader_dev = EXISTReader(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/dev/EXIST2025_dev.json\")\n",
        "reader_test = EXISTReader(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/test/EXIST2025_test_clean.json\")\n",
        "\n",
        "EnTrainTask3, EnDevTask3 = reader_train.get(lang=\"EN\", subtask=\"3\"), reader_dev.get(lang=\"EN\", subtask=\"3\")\n",
        "SpTrainTask3, SpDevTask3 = reader_train.get(lang=\"ES\", subtask=\"3\"), reader_dev.get(lang=\"ES\", subtask=\"3\")\n",
        "\n",
        "SpTestTask3, EnTestTask3 = reader_test.get(lang=\"ES\", subtask=\"3\", include_ambiguous=True),  reader_test.get(lang=\"EN\", subtask=\"3\", include_ambiguous=True)\n",
        "\n",
        "print(EnTrainTask3[1])\n",
        "print(\"-------------------\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwpywTpSytMt"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "zJ6VoJw4yu4_"
      },
      "outputs": [],
      "source": [
        "def clean_text(text_list):\n",
        "    cleaned_corpus = []\n",
        "    for text in text_list:\n",
        "        text = text.lower()\n",
        "        text = re.sub(r\"https?://\\S+\", \"\", text)  # Removes URLs\n",
        "        text = re.sub(r\"@\\w+\", \"\", text)          # Removes mentions\n",
        "        text = text.replace(\"#\", \"\")              # Removes Hashtags\n",
        "        text = re.sub(r\"\\s+\", \" \", text).strip()   # Removes spaces\n",
        "        cleaned_corpus.append(text)\n",
        "    return cleaned_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc7lGppHF70r"
      },
      "source": [
        "### Set Seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "TFRtdrUvF9hd"
      },
      "outputs": [],
      "source": [
        "def set_seed(seed=2025):\n",
        "    # Random seed\n",
        "    random.seed(seed)\n",
        "    # Numpy seed\n",
        "    np.random.seed(seed)\n",
        "    # Torch seed\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    # os seed\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OAQLYdJGHk2"
      },
      "source": [
        "### Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "BZFLQcLtGLEJ"
      },
      "outputs": [],
      "source": [
        "class SexismDatasetMulti(Dataset):\n",
        "    def __init__(self, texts, labels, ids, tokenizer, max_len=128, pad=\"max_length\", trunc=True, rt='pt'):\n",
        "        # if texts is list, just assign it; if numpy/pandas, convert to list\n",
        "        if isinstance(texts, list):\n",
        "            self.texts = texts\n",
        "        else:\n",
        "            self.texts = texts.tolist()\n",
        "            \n",
        "        self.labels = labels\n",
        "        self.ids = ids\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.pad = pad\n",
        "        self.trunc = trunc\n",
        "        self.rt = rt\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding=self.pad,\n",
        "            truncation=self.trunc,\n",
        "            return_tensors=self.rt\n",
        "        )\n",
        "        \n",
        "        item = {\n",
        "            'input_ids': inputs['input_ids'].flatten(),\n",
        "            'attention_mask': inputs['attention_mask'].flatten(),\n",
        "            'id': self.ids[idx]\n",
        "        }\n",
        "\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "\n",
        "        return item\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ICM Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def ICMWrapper(pred, labels, multi=False,ids=None):\n",
        "    test = PyEvALLEvaluation()\n",
        "    metrics=[MetricFactory.ICM.value]\n",
        "    params= dict()\n",
        "    fillLabel=None\n",
        "    if multi:\n",
        "        params[PyEvALLUtils.PARAM_REPORT]=\"embedded\"\n",
        "        hierarchy={\"True\":['IDEOLOGICAL-INEQUALITY', 'STEREOTYPING-DOMINANCE', 'MISOGYNY-NON-SEXUAL-VIOLENCE', 'OBJECTIFICATION', 'SEXUAL-VIOLENCE'],\n",
        "        \"False\":[]}\n",
        "        params[PyEvALLUtils.PARAM_HIERARCHY]=hierarchy\n",
        "        fillLabel = lambda x: [\"False\"] if len(x)== 0 else x\n",
        "    else:\n",
        "        params[PyEvALLUtils.PARAM_REPORT]=\"simple\"\n",
        "        fillLabel = lambda x: str(x)\n",
        "\n",
        "\n",
        "    truth_name, predict_name=None, None\n",
        "    if ids is None:\n",
        "        ids=list(range(len(labels)))\n",
        "\n",
        "    with tempfile.NamedTemporaryFile(mode='w', delete=False, encoding='utf-8') as truth:\n",
        "        truth_name=truth.name\n",
        "        truth_df=pd.DataFrame({'test_case': ['EXIST2025']*len(labels),\n",
        "                        'id': [str(x) for x in ids],\n",
        "                        'value': [fillLabel(x) for x in labels]})\n",
        "        if multi==True:\n",
        "            truth_df=truth_df.astype('object')\n",
        "        truth.write(truth_df.to_json(orient=\"records\"))\n",
        "\n",
        "    with  tempfile.NamedTemporaryFile(mode='w', delete=False) as predict:\n",
        "        predict_name=predict.name\n",
        "        predict_df=pd.DataFrame({'test_case': ['EXIST2025']*len(pred),\n",
        "                        'id': [str(x) for x in ids],\n",
        "                        'value': [fillLabel(x) for x in pred]})\n",
        "        if multi==True:\n",
        "            predict_df=predict_df.astype('object')\n",
        "        predict.write(predict_df.to_json(orient=\"records\"))\n",
        "\n",
        "    report = test.evaluate(predict_name, truth_name, metrics, **params)\n",
        "    os.unlink(truth_name)\n",
        "    os.unlink(predict_name)\n",
        "\n",
        "    icm = None\n",
        "    if 'metrics' in report.report:\n",
        "        if 'ICM' in report.report[\"metrics\"]: icm=float(report.report[\"metrics\"]['ICM'][\"results\"][\"average_per_test_case\"])\n",
        "    return icm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI6FsQ46GNqC"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "S6vS9AKYGPBx"
      },
      "outputs": [],
      "source": [
        "def compute_metrics_3(pred, lencoder):\n",
        "    labels = pred.label_ids\n",
        "    #preds = pred.predictions.argmax(-1)\n",
        "    preds = torch.sigmoid(torch.tensor(pred.predictions)).numpy()\n",
        "    preds_binary = (preds >= 0.5).astype(int)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds_binary, average=None, zero_division=0\n",
        "    )\n",
        "    acc = accuracy_score(labels, preds_binary)\n",
        "    icm= ICMWrapper(lencoder.inverse_transform(preds_binary), lencoder.inverse_transform(labels), multi=True)\n",
        "    # Macro averages\n",
        "    precision_macro = np.mean(precision)\n",
        "    recall_macro = np.mean(recall)\n",
        "    f1_macro = np.mean(f1)\n",
        "    metrics = {}\n",
        "    metrics.update({\n",
        "        'precision_macro': precision_macro,\n",
        "        'recall_macro': recall_macro,\n",
        "        'f1_macro': f1_macro,\n",
        "        'ICM': icm\n",
        "    })\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcML21DcGWB9"
      },
      "source": [
        "### Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "bTNGVypJGXwn"
      },
      "outputs": [],
      "source": [
        "def sexism_classification_pipeline_task3(trainInfo, devInfo, testInfo=None, model_name='bert-base-uncased', nlabels=5, ptype=\"multi_label_classification\", **args):\n",
        "    # Model and Tokenizer\n",
        "    labelEnc= MultiLabelBinarizer()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=nlabels,\n",
        "        problem_type=ptype,\n",
        "        ignore_mismatched_sizes=True)\n",
        "\n",
        "    # Prepare datasets\n",
        "    train_dataset = SexismDatasetMulti(trainInfo[1], labelEnc.fit_transform(trainInfo[2]),[int(x) for x in trainInfo[0]], tokenizer )\n",
        "    val_dataset = SexismDatasetMulti(devInfo[1], labelEnc.transform(devInfo[2]), [int(x) for x in devInfo[0]], tokenizer)\n",
        "\n",
        "    # Training Arguments\n",
        "    training_args = TrainingArguments(\n",
        "        report_to=\"none\", # alt: \"wandb\", \"tensorboard\" \"comet_ml\" \"mlflow\" \"clearml\"\n",
        "        output_dir= args.get('output_dir', './results'),\n",
        "        num_train_epochs= args.get('num_train_epochs', 5),\n",
        "        learning_rate=args.get('learning_rate', 5e-5),\n",
        "        per_device_train_batch_size=args.get('per_device_train_batch_size', 16),\n",
        "        per_device_eval_batch_size=args.get('per_device_eval_batch_size', 64),\n",
        "        warmup_steps=args.get('warmup_steps', 500),\n",
        "        weight_decay=args.get('weight_decay',0.01),\n",
        "        logging_dir=args.get('logging_dir', './logs'),\n",
        "        logging_steps=args.get('logging_steps', 10),\n",
        "        eval_strategy=args.get('eval_strategy','epoch'),\n",
        "        save_strategy=args.get('save_strategy', \"epoch\"),\n",
        "        save_total_limit=args.get('save_total_limit', 1),\n",
        "        load_best_model_at_end=args.get('load_best_model_at_end', True),\n",
        "        metric_for_best_model=args.get('metric_for_best_model',\"ICM\")\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        #compute_metrics=compute_metrics_3,\n",
        "        compute_metrics = partial(compute_metrics_3, lencoder=labelEnc),\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=args.get(\"early_stopping_patience\",10))]\n",
        "    )\n",
        "\n",
        "    # Fine-tune the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"Validation Results:\", eval_results)\n",
        "\n",
        "    if testInfo is not None:\n",
        "      # Prepare test dataset for prediction\n",
        "      test_dataset = SexismDatasetMulti(testInfo[1], [[0,0,0,0,0]] * len(testInfo[1]),  [int(x) for x in testInfo[0]],   tokenizer)\n",
        "\n",
        "      # Predict test set labels\n",
        "      predictions = trainer.predict(test_dataset)\n",
        "      #predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "      predicted_probs = torch.sigmoid(torch.tensor(predictions.predictions)).numpy()\n",
        "      predicted_labels = (predicted_probs >= 0.5).astype(int)\n",
        "\n",
        "      # Create submission DataFrame\n",
        "      submission_df = pd.DataFrame({\n",
        "          'id': testInfo[0],\n",
        "          'label': labelEnc.inverse_transform(predicted_labels),\n",
        "          \"test_case\": [\"EXIST2025\"]*len(predicted_labels)\n",
        "\n",
        "      })\n",
        "      submission_df.to_csv('sexism_predictions_task3.csv', index=False)\n",
        "      print(\"Prediction TASK3 completed. Results saved to sexism_predictions_task3.csv\")\n",
        "      return model, submission_df\n",
        "    return model, eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dp_oaw2Rxed"
      },
      "source": [
        "### Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(23)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "collapsed": true,
        "id": "_qxgo8SqX-cV",
        "outputId": "a65ee470-ab56-408e-84d8-ba931cace0ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='655' max='655' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [655/655 08:02, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision Macro</th>\n",
              "      <th>Recall Macro</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>Icm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.672400</td>\n",
              "      <td>0.664214</td>\n",
              "      <td>0.479039</td>\n",
              "      <td>0.749451</td>\n",
              "      <td>0.584449</td>\n",
              "      <td>-0.129416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.645100</td>\n",
              "      <td>0.646880</td>\n",
              "      <td>0.509034</td>\n",
              "      <td>0.596494</td>\n",
              "      <td>0.519975</td>\n",
              "      <td>-0.452479</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.626400</td>\n",
              "      <td>0.616054</td>\n",
              "      <td>0.729256</td>\n",
              "      <td>0.583306</td>\n",
              "      <td>0.595817</td>\n",
              "      <td>-0.785745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.575300</td>\n",
              "      <td>0.580942</td>\n",
              "      <td>0.695196</td>\n",
              "      <td>0.762700</td>\n",
              "      <td>0.722180</td>\n",
              "      <td>0.136201</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.517800</td>\n",
              "      <td>0.581358</td>\n",
              "      <td>0.723532</td>\n",
              "      <td>0.667992</td>\n",
              "      <td>0.687315</td>\n",
              "      <td>-0.526377</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-23 18:22:08,121 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
            "2025-05-23 18:22:08,149 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-23 18:23:43,137 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
            "2025-05-23 18:23:43,163 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-23 18:25:19,479 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
            "2025-05-23 18:25:19,508 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-23 18:26:57,283 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
            "2025-05-23 18:26:57,320 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-23 18:28:36,764 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
            "2025-05-23 18:28:36,787 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-23 18:28:43,245 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
            "2025-05-23 18:28:43,271 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
            "Validation Results: {'eval_loss': 0.5809418559074402, 'eval_precision_macro': 0.6951963388164534, 'eval_recall_macro': 0.7626999216170279, 'eval_f1_macro': 0.7221801910470803, 'eval_ICM': 0.13620083876419975, 'eval_runtime': 4.4487, 'eval_samples_per_second': 75.978, 'eval_steps_per_second': 1.349, 'epoch': 5.0}\n"
          ]
        }
      ],
      "source": [
        "model = \"bert-base-uncased\"\n",
        "\n",
        "params = {\n",
        "    \"num_train_epochs\": 5,\n",
        "    \"per_device_train_batch_size\": 16,\n",
        "    \"per_device_eval_batch_size\": 64,\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"early_stopping_patience\": 5,\n",
        "    \"output_dir\": \"./bert_results\"\n",
        "}\n",
        "model_bert_en, results_bert_en = sexism_classification_pipeline_task3(\n",
        "    EnTrainTask3,\n",
        "    EnDevTask3,\n",
        "    testInfo=None,\n",
        "    model_name=model,\n",
        "    **params\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validation Results: {'eval_loss': 0.5809418559074402, 'eval_precision_macro': 0.6951963388164534, 'eval_recall_macro': 0.7626999216170279, 'eval_f1_macro': 0.7221801910470803, 'eval_ICM': 0.13620083876419975, 'eval_runtime': 4.4487, 'eval_samples_per_second': 75.978, 'eval_steps_per_second': 1.349, 'epoch': 5.0}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "yc7ZAvPOH7f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 0.5809418559074402, 'eval_precision_macro': 0.6951963388164534, 'eval_recall_macro': 0.7626999216170279, 'eval_f1_macro': 0.7221801910470803, 'eval_ICM': 0.13620083876419975, 'eval_runtime': 4.4487, 'eval_samples_per_second': 75.978, 'eval_steps_per_second': 1.349, 'epoch': 5.0}\n"
          ]
        }
      ],
      "source": [
        "print(results_bert_en)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"bert-base-uncased\"  # or the model you actually used\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('models/bert_sexism/tokenizer_config.json',\n",
              " 'models/bert_sexism/special_tokens_map.json',\n",
              " 'models/bert_sexism/vocab.txt',\n",
              " 'models/bert_sexism/added_tokens.json',\n",
              " 'models/bert_sexism/tokenizer.json')"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_bert_en.save_pretrained(\"models/bert_sexism\")\n",
        "tokenizer.save_pretrained(\"models/bert_sexism\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get previous YES statement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_ids(yes_ids):\n",
        "    with open(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/test/EXIST2025_test_clean.json\", \"r\") as f:\n",
        "        test_json = json.load(f)\n",
        "\n",
        "    # Filter to only English tweets\n",
        "    english_tweets = [item for item in test_json.values() if item[\"lang\"] == \"en\"]\n",
        "\n",
        "    # YES tweets (IDs present in yes_ids)\n",
        "    yes_tweets = [t[\"tweet\"] for t in english_tweets if t[\"id_EXIST\"] in yes_ids]\n",
        "    yes_ids_filtered = [t[\"id_EXIST\"] for t in english_tweets if t[\"id_EXIST\"] in yes_ids]\n",
        "\n",
        "    # NO tweets (IDs not in yes_ids)\n",
        "    no_tweets = [t[\"tweet\"] for t in english_tweets if t[\"id_EXIST\"] not in yes_ids]\n",
        "    no_ids_filtered = [t[\"id_EXIST\"] for t in english_tweets if t[\"id_EXIST\"] not in yes_ids]\n",
        "\n",
        "    print(f\"Total English tweets in test set: {len(english_tweets)}\")\n",
        "    print(f\"English tweets predicted YES: {len(yes_tweets)}\")\n",
        "    print(f\"English tweets predicted NO: {len(no_tweets)}\")\n",
        "\n",
        "    return yes_ids_filtered, yes_tweets, no_ids_filtered, no_tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['600002', '600004', '600005', '600006', '600013']\n"
          ]
        }
      ],
      "source": [
        "# Load your BoW predictions from the JSON file\n",
        "with open(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/Task 1.1/Bert/bert_task1_submission.json\", \"r\") as f:\n",
        "    bert_results = json.load(f)\n",
        "\n",
        "# Extract the IDs that were predicted as \"YES\"\n",
        "yes_ids_bert = [entry[\"id\"] for entry in bert_results if entry[\"value\"] == \"YES\"]\n",
        "yes_ids_bert = [id_.replace(\"id_\", \"\") for id_ in yes_ids_bert]\n",
        "\n",
        "\n",
        "print(yes_ids_bert[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total English tweets in test set: 978\n",
            "English tweets predicted YES: 430\n",
            "English tweets predicted NO: 548\n",
            "Tweet: @Cathy_TwoHalves @EverydaySexism That's why women have to stay at home, raise children and take care of the house.Nobody is going to assume that a woman is a leader among men since that's not how it is supposed to be in nature. Of course, you can try and fake it, but you will keep losing your feminine side.\n",
            "Predicted labels: ('IDEOLOGICAL-INEQUALITY', 'MISOGYNY-NON-SEXUAL-VIOLENCE', 'STEREOTYPING-DOMINANCE')\n",
            "\n",
            "Tweet: @JonPincince @kmulvane @EverydaySexism, even after Roe v. Wade was overturned. Fun! /s\n",
            "Predicted labels: ('IDEOLOGICAL-INEQUALITY',)\n",
            "\n",
            "Tweet: Seriously @JohnLewisRetail?  girls are pictured with ‘I’m a Stylist’ doll and the pizza oven - the boys get a robot and hot wheels garage. #EverydaySexism https://t.co/mKsbxaGgIV? # via @HPUKParents\n",
            "Predicted labels: ('IDEOLOGICAL-INEQUALITY', 'MISOGYNY-NON-SEXUAL-VIOLENCE', 'OBJECTIFICATION', 'STEREOTYPING-DOMINANCE')\n",
            "\n",
            "Tweet: not a great look for the French Cycling Federation #everydaysexism https://t.co/Ts83OMDOgL\n",
            "Predicted labels: ('IDEOLOGICAL-INEQUALITY',)\n",
            "\n",
            "Tweet: @tired_of_debate @ScoutSleepe @ImWatson91 The fact that some men assault transwomen too is awful &amp; unacceptable.Having said that—being harassed by men does NOT render MtFs “women” (+men *do* harass other men sometimes, too…)+It is notable that—for ALL sexual harassment/assault victims—the abuser is almost always MALE!\n",
            "Predicted labels: ('IDEOLOGICAL-INEQUALITY', 'MISOGYNY-NON-SEXUAL-VIOLENCE', 'OBJECTIFICATION', 'SEXUAL-VIOLENCE', 'STEREOTYPING-DOMINANCE')\n",
            "\n"
          ]
        }
      ],
      "source": [
        "filtered_ids, filtered_tweets, no_ids, no_tweets = get_ids(yes_ids_bert)\n",
        "\n",
        "cleaned_filtered_tweets = clean_text(filtered_tweets)\n",
        "\n",
        "# --- 2. Load model + tokenizer + multi-label binarizer ---\n",
        "model_path = \"models/bert_sexism\"  # update path to your multi-label model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "# MultiLabelBinarizer with all possible task 3 labels\n",
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit([\n",
        "    [\"OBJECTIFICATION\"], \n",
        "    [\"SEXUAL-VIOLENCE\"],\n",
        "    [\"STEREOTYPING-DOMINANCE\"],\n",
        "    [\"IDEOLOGICAL-INEQUALITY\"],\n",
        "    [\"MISOGYNY-NON-SEXUAL-VIOLENCE\"]\n",
        "    # Add all labels as needed\n",
        "])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# --- 4. Create dataset & dataloader ---\n",
        "test_dataset = SexismDatasetMulti(\n",
        "    texts=filtered_tweets,\n",
        "    labels=None,               # No labels in inference\n",
        "    ids=filtered_ids,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=128\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# --- 5. Predict multi-label outputs ---\n",
        "all_preds = []\n",
        "\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probs = sigmoid(logits)  # Sigmoid to get probabilities per label\n",
        "\n",
        "        # Apply threshold (e.g., 0.5) to decide which labels apply\n",
        "        batch_preds = (probs > 0.5).cpu().numpy()\n",
        "        all_preds.extend(batch_preds)\n",
        "\n",
        "# --- 6. Decode multi-label predictions ---\n",
        "# Convert list of predictions to numpy array\n",
        "all_preds_array = np.array(all_preds)\n",
        "pred_labels = mlb.inverse_transform(all_preds_array)\n",
        "\n",
        "# Optional: print first 5 predictions\n",
        "for tweet, labels in zip(filtered_tweets[:5], pred_labels[:5]):\n",
        "    print(f\"Tweet: {tweet}\\nPredicted labels: {labels}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save to json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved predictions including NO tweets to 'bert_task3_submission.json'\n"
          ]
        }
      ],
      "source": [
        "# After decoding pred_labels for the YES tweets\n",
        "output_json = []\n",
        "\n",
        "# YES predictions\n",
        "for tweet_id, label in zip(filtered_ids, pred_labels):\n",
        "    output_json.append({\n",
        "        \"id\": str(tweet_id),\n",
        "        \"value\": label,\n",
        "        \"test_case\": \"EXIST2025\"\n",
        "    })\n",
        "\n",
        "# NO predictions (no task 2 value assigned)\n",
        "for tweet_id in no_ids:\n",
        "    output_json.append({\n",
        "        \"id\": str(tweet_id),\n",
        "        \"value\": \"NO\",  # Placeholder\n",
        "        \"test_case\": \"EXIST2025\"\n",
        "    })\n",
        "\n",
        "# Optional: Sort by ID\n",
        "output_json = sorted(output_json, key=lambda x: int(x[\"id\"]))\n",
        "\n",
        "# Save\n",
        "with open(\"bert_task3_submission.json\", \"w\") as f:\n",
        "    json.dump(output_json, f, indent=4)\n",
        "\n",
        "print(\"Saved predictions including NO tweets to 'bert_task3_submission.json'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Spanish"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sexism_classification_pipeline_task3_spanish(trainInfo, devInfo, testInfo=None, model_name='dccuchile/bert-base-spanish-wwm-cased', nlabels=5, ptype=\"multi_label_classification\", **args):\n",
        "    # Model and Tokenizer\n",
        "    labelEnc= MultiLabelBinarizer()\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=nlabels,\n",
        "        problem_type=ptype,\n",
        "        ignore_mismatched_sizes=True)\n",
        "\n",
        "    # Prepare datasets\n",
        "    train_dataset = SexismDatasetMulti(trainInfo[1], labelEnc.fit_transform(trainInfo[2]),[int(x) for x in trainInfo[0]], tokenizer )\n",
        "    val_dataset = SexismDatasetMulti(devInfo[1], labelEnc.transform(devInfo[2]), [int(x) for x in devInfo[0]], tokenizer)\n",
        "\n",
        "    # Training Arguments\n",
        "    training_args = TrainingArguments(\n",
        "        report_to=\"none\", # alt: \"wandb\", \"tensorboard\" \"comet_ml\" \"mlflow\" \"clearml\"\n",
        "        output_dir= args.get('output_dir', './results'),\n",
        "        num_train_epochs= args.get('num_train_epochs', 5),\n",
        "        learning_rate=args.get('learning_rate', 5e-5),\n",
        "        per_device_train_batch_size=args.get('per_device_train_batch_size', 16),\n",
        "        per_device_eval_batch_size=args.get('per_device_eval_batch_size', 64),\n",
        "        warmup_steps=args.get('warmup_steps', 500),\n",
        "        weight_decay=args.get('weight_decay',0.01),\n",
        "        logging_dir=args.get('logging_dir', './logs'),\n",
        "        logging_steps=args.get('logging_steps', 10),\n",
        "        eval_strategy=args.get('eval_strategy','epoch'),\n",
        "        save_strategy=args.get('save_strategy', \"epoch\"),\n",
        "        save_total_limit=args.get('save_total_limit', 1),\n",
        "        load_best_model_at_end=args.get('load_best_model_at_end', True),\n",
        "        metric_for_best_model=args.get('metric_for_best_model',\"ICM\")\n",
        "    )\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        #compute_metrics=compute_metrics_3,\n",
        "        compute_metrics = partial(compute_metrics_3, lencoder=labelEnc),\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=args.get(\"early_stopping_patience\",10))]\n",
        "    )\n",
        "\n",
        "    # Fine-tune the model\n",
        "    trainer.train()\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    eval_results = trainer.evaluate()\n",
        "    print(\"Validation Results:\", eval_results)\n",
        "\n",
        "    if testInfo is not None:\n",
        "      # Prepare test dataset for prediction\n",
        "      test_dataset = SexismDatasetMulti(testInfo[1], [[0,0,0,0,0]] * len(testInfo[1]),  [int(x) for x in testInfo[0]],   tokenizer)\n",
        "\n",
        "      # Predict test set labels\n",
        "      predictions = trainer.predict(test_dataset)\n",
        "      #predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
        "      predicted_probs = torch.sigmoid(torch.tensor(predictions.predictions)).numpy()\n",
        "      predicted_labels = (predicted_probs >= 0.5).astype(int)\n",
        "\n",
        "      # Create submission DataFrame\n",
        "      submission_df = pd.DataFrame({\n",
        "          'id': testInfo[0],\n",
        "          'label': labelEnc.inverse_transform(predicted_labels),\n",
        "          \"test_case\": [\"EXIST2025\"]*len(predicted_labels)\n",
        "\n",
        "      })\n",
        "      submission_df.to_csv('sexism_predictions_task3.csv', index=False)\n",
        "      print(\"Prediction TASK3 completed. Results saved to sexism_predictions_task3.csv\")\n",
        "      return model, submission_df\n",
        "    return model, eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='655' max='655' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [655/655 08:55, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision Macro</th>\n",
              "      <th>Recall Macro</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>Icm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.660300</td>\n",
              "      <td>0.667164</td>\n",
              "      <td>0.397337</td>\n",
              "      <td>0.475897</td>\n",
              "      <td>0.410437</td>\n",
              "      <td>-0.830431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.643200</td>\n",
              "      <td>0.650526</td>\n",
              "      <td>0.617245</td>\n",
              "      <td>0.483977</td>\n",
              "      <td>0.441208</td>\n",
              "      <td>-0.734095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.626300</td>\n",
              "      <td>0.628984</td>\n",
              "      <td>0.673536</td>\n",
              "      <td>0.614387</td>\n",
              "      <td>0.636423</td>\n",
              "      <td>-0.464258</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.597000</td>\n",
              "      <td>0.613683</td>\n",
              "      <td>0.673966</td>\n",
              "      <td>0.669319</td>\n",
              "      <td>0.665496</td>\n",
              "      <td>-0.194924</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.540100</td>\n",
              "      <td>0.613738</td>\n",
              "      <td>0.691110</td>\n",
              "      <td>0.651308</td>\n",
              "      <td>0.659788</td>\n",
              "      <td>-0.220338</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-23 18:40:56,566 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
            "2025-05-23 18:40:56,589 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-23 18:42:43,743 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
            "2025-05-23 18:42:43,767 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-23 18:44:32,654 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
            "2025-05-23 18:44:32,682 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-23 18:46:23,128 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
            "2025-05-23 18:46:23,167 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-23 18:48:11,223 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
            "2025-05-23 18:48:11,247 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:03]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-05-23 18:48:17,274 - pyevall.evaluation - INFO -             evaluate() - Evaluating the following metrics ['ICM']\n",
            "2025-05-23 18:48:17,296 - pyevall.metrics.metrics - INFO -             evaluate() - Executing ICM evaluation method\n",
            "Validation Results: {'eval_loss': 0.61368328332901, 'eval_precision_macro': 0.6739660395481791, 'eval_recall_macro': 0.6693191294085508, 'eval_f1_macro': 0.6654963344229851, 'eval_ICM': -0.1949238789607193, 'eval_runtime': 4.1437, 'eval_samples_per_second': 81.571, 'eval_steps_per_second': 1.448, 'epoch': 5.0}\n"
          ]
        }
      ],
      "source": [
        "model = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
        "\n",
        "params = {\n",
        "    \"num_train_epochs\": 5,\n",
        "    \"per_device_train_batch_size\": 16,\n",
        "    \"per_device_eval_batch_size\": 64,\n",
        "    \"learning_rate\": 1e-5,\n",
        "    \"early_stopping_patience\": 5,\n",
        "    \"output_dir\": \"./bert_results\"\n",
        "}\n",
        "model_bert_en, results_bert_en = sexism_classification_pipeline_task3(\n",
        "    EnTrainTask3,\n",
        "    EnDevTask3,\n",
        "    testInfo=None,\n",
        "    model_name=model,\n",
        "    **params\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validation Results: {'eval_loss': 0.61368328332901, 'eval_precision_macro': 0.6739660395481791, 'eval_recall_macro': 0.6693191294085508, 'eval_f1_macro': 0.6654963344229851, 'eval_ICM': -0.1949238789607193, 'eval_runtime': 4.1437, 'eval_samples_per_second': 81.571, 'eval_steps_per_second': 1.448, 'epoch': 5.0}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"  # or the model you actually used\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('models_spanish/bert-base-spanish-wwm-cased/tokenizer_config.json',\n",
              " 'models_spanish/bert-base-spanish-wwm-cased/special_tokens_map.json',\n",
              " 'models_spanish/bert-base-spanish-wwm-cased/vocab.txt',\n",
              " 'models_spanish/bert-base-spanish-wwm-cased/added_tokens.json',\n",
              " 'models_spanish/bert-base-spanish-wwm-cased/tokenizer.json')"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_bert_en.save_pretrained(\"models_spanish/bert-base-spanish-wwm-cased\")\n",
        "tokenizer.save_pretrained(\"models_spanish/bert-base-spanish-wwm-cased\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get previous YES statements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_ids_spanish(yes_ids):\n",
        "    with open(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/test/EXIST2025_test_clean.json\", \"r\") as f:\n",
        "        test_json = json.load(f)\n",
        "\n",
        "    # Spanish tweets only\n",
        "    spanish_tweets = [item for item in test_json.values() if item[\"lang\"] == \"es\"]\n",
        "\n",
        "    # YES tweets\n",
        "    yes_tweets = [t[\"tweet\"] for t in spanish_tweets if t[\"id_EXIST\"] in yes_ids]\n",
        "    yes_ids_filtered = [t[\"id_EXIST\"] for t in spanish_tweets if t[\"id_EXIST\"] in yes_ids]\n",
        "\n",
        "    # NO tweets (not in yes_ids)\n",
        "    no_tweets = [t[\"tweet\"] for t in spanish_tweets if t[\"id_EXIST\"] not in yes_ids]\n",
        "    no_ids_filtered = [t[\"id_EXIST\"] for t in spanish_tweets if t[\"id_EXIST\"] not in yes_ids]\n",
        "\n",
        "    print(f\"Spanish tweets predicted YES: {len(yes_tweets)}\")\n",
        "    print(f\"Spanish tweets predicted NO: {len(no_tweets)}\")\n",
        "\n",
        "    return yes_ids_filtered, yes_tweets, no_ids_filtered, no_tweets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['500004', '500012', '500019', '500020', '500022']\n"
          ]
        }
      ],
      "source": [
        "# Load your BoW predictions from the JSON file\n",
        "with open(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/Task 1.1/Bert/bert_task1_submission_Spanish.json\", \"r\") as f:\n",
        "    bert_results = json.load(f)\n",
        "\n",
        "# Extract the IDs that were predicted as \"YES\"\n",
        "yes_ids_bert = [entry[\"id\"] for entry in bert_results if entry[\"value\"] == \"YES\"]\n",
        "yes_ids_bert = [id_.replace(\"id_\", \"\") for id_ in yes_ids_bert]\n",
        "\n",
        "\n",
        "print(yes_ids_bert[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spanish tweets predicted YES: 567\n",
            "Spanish tweets predicted NO: 531\n",
            "Tweet: @jordirico Primero fue internet, luego el gamergate, la manosfera y su misoginia extrema sin que las plataformas movieran un dedo, los incel-asesinatos, la extrema derecha capitalizando el odio, la mimetización de estas ideas con las de un debate respetable y finalmente aquí estamos\n",
            "Predicted labels: ()\n",
            "\n",
            "Tweet: @dimplerrylover lo se pero literalmente la chica estaba siendo harassed like como puede sonreir asi\n",
            "Predicted labels: ('MISOGYNY-NON-SEXUAL-VIOLENCE', 'OBJECTIFICATION', 'SEXUAL-VIOLENCE', 'STEREOTYPING-DOMINANCE')\n",
            "\n",
            "Tweet: @Fistroman1 @ElioGatsby Sin pruebas??Qué fue en público.Qué lo vimos todos, qué me estás contando.Que no me mezcles churras con merinas.Qué de aquí a que me vengas con el metoo y el notallmen queda poquito y no me apetece acabar discutiendo de verdad, q lo veo venir.\n",
            "Predicted labels: ('OBJECTIFICATION', 'SEXUAL-VIOLENCE', 'STEREOTYPING-DOMINANCE')\n",
            "\n",
            "Tweet: @EstefaniaVeloz ...Con el proceso legal, no para hacer arder a una persona (No sé ni quienes sean en este caso). Hay mujeres que son víctimas pero también hay corruptas, violentas y victimarias. El movimiento #MeToo ya cumplió su cometido ¿Qué sigue? ¿Cómo avanzar sin que sea un arma a capricho?\n",
            "Predicted labels: ('IDEOLOGICAL-INEQUALITY', 'OBJECTIFICATION', 'STEREOTYPING-DOMINANCE')\n",
            "\n",
            "Tweet: @geeksterilia Desde la Olivia que anda trepadota en ladrillo por ser directora.El video de la llama a Shia está así de la wey toda condescendiente para MI floQue para esto según ella le pidió a Shia retirarse de la pelo porque pues metooY Andale que nada. La wey fingiendo https://t.co/t9zCKx1IDs\n",
            "Predicted labels: ('OBJECTIFICATION', 'SEXUAL-VIOLENCE', 'STEREOTYPING-DOMINANCE')\n",
            "\n"
          ]
        }
      ],
      "source": [
        "filtered_ids, filtered_tweets, no_ids, no_tweets = get_ids_spanish(yes_ids_bert)\n",
        "\n",
        "cleaned_filtered_tweets = clean_text(filtered_tweets)\n",
        "\n",
        "# --- 2. Load model + tokenizer + multi-label binarizer ---\n",
        "model_path = \"models_spanish/bert-base-spanish-wwm-cased\"  # update path to your multi-label model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "# MultiLabelBinarizer with all possible task 3 labels\n",
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit([\n",
        "    [\"OBJECTIFICATION\"], \n",
        "    [\"SEXUAL-VIOLENCE\"],\n",
        "    [\"STEREOTYPING-DOMINANCE\"],\n",
        "    [\"IDEOLOGICAL-INEQUALITY\"],\n",
        "    [\"MISOGYNY-NON-SEXUAL-VIOLENCE\"]\n",
        "    # Add all labels as needed\n",
        "])\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# --- 4. Create dataset & dataloader ---\n",
        "test_dataset = SexismDatasetMulti(\n",
        "    texts=filtered_tweets,\n",
        "    labels=None,               # No labels in inference\n",
        "    ids=filtered_ids,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=128\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# --- 5. Predict multi-label outputs ---\n",
        "all_preds = []\n",
        "\n",
        "sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probs = sigmoid(logits)  # Sigmoid to get probabilities per label\n",
        "\n",
        "        # Apply threshold (e.g., 0.5) to decide which labels apply\n",
        "        batch_preds = (probs > 0.5).cpu().numpy()\n",
        "        all_preds.extend(batch_preds)\n",
        "\n",
        "# --- 6. Decode multi-label predictions ---\n",
        "all_preds_array = np.array(all_preds)\n",
        "pred_labels = mlb.inverse_transform(all_preds_array)\n",
        "\n",
        "# Optional: print first 5 predictions\n",
        "for tweet, labels in zip(filtered_tweets[:5], pred_labels[:5]):\n",
        "    print(f\"Tweet: {tweet}\\nPredicted labels: {labels}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save to json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved predictions including NO tweets to 'bert_task3_submission_Spanish.json'\n"
          ]
        }
      ],
      "source": [
        "# After decoding pred_labels for the YES tweets\n",
        "output_json = []\n",
        "\n",
        "# YES predictions\n",
        "for tweet_id, label in zip(filtered_ids, pred_labels):\n",
        "    output_json.append({\n",
        "        \"id\": str(tweet_id),\n",
        "        \"value\": label,\n",
        "        \"test_case\": \"EXIST2025\"\n",
        "    })\n",
        "\n",
        "# NO predictions (no task 2 value assigned)\n",
        "for tweet_id in no_ids:\n",
        "    output_json.append({\n",
        "        \"id\": str(tweet_id),\n",
        "        \"value\": \"NO\",  # Placeholder\n",
        "        \"test_case\": \"EXIST2025\"\n",
        "    })\n",
        "\n",
        "# Optional: Sort by ID\n",
        "output_json = sorted(output_json, key=lambda x: int(x[\"id\"]))\n",
        "\n",
        "# Save\n",
        "with open(\"bert_task3_submission_Spanish.json\", \"w\") as f:\n",
        "    json.dump(output_json, f, indent=4)\n",
        "\n",
        "print(\"Saved predictions including NO tweets to 'bert_task3_submission_Spanish.json'\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### File Merge"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merged 978 EN + 1098 ES = 2076 total predictions.\n"
          ]
        }
      ],
      "source": [
        "filename_english = \"bert_task3_submission.json\"\n",
        "filename_spanish = \"bert_task3_submission_Spanish.json\"\n",
        "filename_merged = \"bert_task3_submission_merge.json\"\n",
        "\n",
        "def merge_predictions(filename_english, filename_spanish, filename_merged):\n",
        "    # Load English predictions\n",
        "    with open(filename_english, \"r\", encoding=\"utf-8\") as f_en:\n",
        "        preds_en = json.load(f_en)\n",
        "\n",
        "    # Load Spanish predictions\n",
        "    with open(filename_spanish, \"r\", encoding=\"utf-8\") as f_es:\n",
        "        preds_es = json.load(f_es)\n",
        "\n",
        "    # Merge the two lists\n",
        "    merged_preds = preds_es + preds_en\n",
        "\n",
        "    # Save the combined predictions\n",
        "    with open(filename_merged, \"w\", encoding=\"utf-8\") as f_out:\n",
        "        json.dump(merged_preds, f_out, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"Merged {len(preds_en)} EN + {len(preds_es)} ES = {len(merged_preds)} total predictions.\")\n",
        "    \n",
        "    \n",
        "merge_predictions(\n",
        "    filename_english,\n",
        "    filename_spanish,\n",
        "    filename_merged\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
