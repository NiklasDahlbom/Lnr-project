{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eb3010d",
   "metadata": {},
   "source": [
    "### Lnr Project Task 1.3 #  Logistic Regression + TF-IDF (Bag of words)\n",
    "\n",
    "Niklas Dahlbom, ndahlbom@kth.se, ndahlbo@upv.edu.es"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394d9fa2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f2629d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from readerEXIST2025 import EXISTReader\n",
    "import re\n",
    "import json\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a64b96",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5ec88115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1       Writing a uni essay in my local pub with a cof...\n",
      "2       @UniversalORL it is 2021 not 1921. I dont appr...\n",
      "5       According to a customer I have plenty of time ...\n",
      "6       So only 'blokes' drink beer? Sorry, but if you...\n",
      "7       New to the shelves this week - looking forward...\n",
      "                              ...                        \n",
      "3255    idk why y’all bitches think having half your a...\n",
      "3256    This has been a part of an experiment with @Wo...\n",
      "3257    \"Take me already\" \"Not yet. You gotta be ready...\n",
      "3258    @clintneedcoffee why do you look like a whore?...\n",
      "3259    ik when mandy says “you look like a whore” i l...\n",
      "Name: text, Length: 2095, dtype: object\n",
      "-------------------\n"
     ]
    }
   ],
   "source": [
    "reader_train = EXISTReader(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/training/EXIST2025_training.json\")\n",
    "reader_dev = EXISTReader(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/dev/EXIST2025_dev.json\")\n",
    "reader_test = EXISTReader(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/test/EXIST2025_test_clean.json\")\n",
    "\n",
    "EnTrainTask3, EnDevTask3 = reader_train.get(lang=\"EN\", subtask=\"3\"), reader_dev.get(lang=\"EN\", subtask=\"3\")\n",
    "SpTrainTask3, SpDevTask3 = reader_train.get(lang=\"ES\", subtask=\"3\"), reader_dev.get(lang=\"ES\", subtask=\"3\")\n",
    "\n",
    "SpTestTask3, EnTestTask3 = reader_test.get(lang=\"ES\", subtask=\"3\", include_ambiguous=True),  reader_test.get(lang=\"EN\", subtask=\"3\", include_ambiguous=True)\n",
    "\n",
    "print(EnTrainTask3[1])\n",
    "print(\"-------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36086650",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e249fa4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text_list):\n",
    "    cleaned_corpus = []\n",
    "    for text in text_list:\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"https?://\\S+\", \"\", text)  # Removes URLs\n",
    "        text = re.sub(r\"@\\w+\", \"\", text)          # Removes mentions\n",
    "        text = text.replace(\"#\", \"\")              # Removes Hashtags\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()   # Removes spaces\n",
    "        cleaned_corpus.append(text)\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "08c2bb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, multilabel_confusion_matrix\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    print(f\"Accuracy (subset accuracy): {acc:.3f}\")\n",
    "    print(f\"Macro F1: {f1:.3f}\")\n",
    "    print(f\"Macro Precision: {precision:.3f}\")\n",
    "    print(f\"Macro Recall: {recall:.3f}\")\n",
    "\n",
    "    # Compute multilabel confusion matrix\n",
    "    icm_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Sum TP, FP, FN across all classes\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    for matrix in icm_matrices:\n",
    "        tn, fp, fn, tp = matrix.ravel()\n",
    "        TP += tp\n",
    "        FP += fp\n",
    "        FN += fn\n",
    "\n",
    "    icm_value = TP / (TP + FP + FN) if (TP + FP + FN) > 0 else 0\n",
    "    print(f\"Singular ICM value (overall label accuracy): {icm_value:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bbf769a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions_to_json(yes_ids, yes_labels, output_path, lang=\"en\"):\n",
    "    import json\n",
    "\n",
    "    # Ensure yes_ids are strings\n",
    "    yes_ids = [str(id_).replace(\"id_\", \"\") for id_ in yes_ids]\n",
    "\n",
    "    # Load test data\n",
    "    with open(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/test/EXIST2025_test_clean.json\", \"r\") as f:\n",
    "        test_json = json.load(f)\n",
    "\n",
    "    # Get all IDs for the selected language\n",
    "    all_ids = [\n",
    "        str(tweet_data[\"id_EXIST\"])\n",
    "        for tweet_data in test_json.values()\n",
    "        if tweet_data[\"lang\"] == lang\n",
    "    ]\n",
    "\n",
    "    # Determine NO IDs\n",
    "    yes_ids_set = set(yes_ids)\n",
    "    all_ids_set = set(all_ids)\n",
    "    no_ids = list(all_ids_set - yes_ids_set)\n",
    "\n",
    "    # Build output JSON\n",
    "    output_data = []\n",
    "\n",
    "    # Add YES predictions\n",
    "    for tweet_id, label in zip(yes_ids, yes_labels):\n",
    "        output_data.append({\n",
    "            \"id\": tweet_id,\n",
    "            \"value\": label,\n",
    "            \"test_case\": \"EXIST2025\"\n",
    "        })\n",
    "\n",
    "    # Add NO predictions\n",
    "    for tweet_id in no_ids:\n",
    "        output_data.append({\n",
    "            \"id\": tweet_id,\n",
    "            \"value\": \"NO\",\n",
    "            \"test_case\": \"EXIST2025\"\n",
    "        })\n",
    "\n",
    "    # Sort by ID numerically\n",
    "    output_data.sort(key=lambda x: int(x[\"id\"]))\n",
    "\n",
    "    # Save to file\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Saved {len(output_data)} predictions to {output_path} (YES: {len(yes_ids)}, NO: {len(no_ids)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2dc07d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['IDEOLOGICAL-INEQUALITY' 'MISOGYNY-NON-SEXUAL-VIOLENCE' 'OBJECTIFICATION'\n",
      " 'SEXUAL-VIOLENCE' 'STEREOTYPING-DOMINANCE']\n",
      "Encoded shape: (2095, 5)\n"
     ]
    }
   ],
   "source": [
    "# === 2. Extract training and validation data ===\n",
    "training_tweets_raw = EnTrainTask3[1]      # Raw tweet texts from the training set\n",
    "training_labels_raw = EnTrainTask3[2]      # Corresponding labels for training tweets\n",
    "\n",
    "validation_tweets_raw = EnDevTask3[1]      # Raw tweet texts from the validation set\n",
    "validation_labels_raw = EnDevTask3[2]      # Corresponding labels for validation tweets\n",
    "\n",
    "# === 3. Clean the tweets (e.g., remove URLs, mentions, special characters) ===\n",
    "cleaned_training_tweets = clean_text(training_tweets_raw)\n",
    "cleaned_validation_tweets = clean_text(validation_tweets_raw)\n",
    "\n",
    "# Initialize MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit and transform on training labels\n",
    "encoded_training_labels = mlb.fit_transform(training_labels_raw)\n",
    "\n",
    "# Transform validation labels using the same mlb (no refitting)\n",
    "encoded_validation_labels = mlb.transform(validation_labels_raw)\n",
    "\n",
    "print(\"Classes:\", mlb.classes_)\n",
    "print(\"Encoded shape:\", encoded_training_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94893496",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780e6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (subset accuracy): 0.136\n",
      "Macro F1: 0.652\n",
      "Macro Precision: 0.699\n",
      "Macro Recall: 0.614\n",
      "Singular ICM value (overall label accuracy): 0.492\n",
      "Note: This evaluation is performed on the full validation set, not just tweets predicted as YES by previous models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n"
     ]
    }
   ],
   "source": [
    "# === 4. Encode string labels as multi-label binarized vectors ===\n",
    "mlb = MultiLabelBinarizer()\n",
    "encoded_training_labels = mlb.fit_transform(training_labels_raw)\n",
    "encoded_validation_labels = mlb.transform(validation_labels_raw)\n",
    "\n",
    "# === 5. Convert cleaned text data into TF-IDF features ===\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))  # unigrams and bigrams\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(cleaned_training_tweets)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(cleaned_validation_tweets)\n",
    "\n",
    "# === 6. Train a One-vs-Rest Logistic Regression classifier for multi-label ===\n",
    "logistic_regressor = R(LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "logistic_regressor.fit(X_train_tfidf, encoded_training_labels)\n",
    "\n",
    "# === 7. Predict and evaluate on the validation set ===\n",
    "validation_predictions = logistic_regressor.predict(X_val_tfidf)\n",
    "\n",
    "# Calculate and display custom evaluation metrics\n",
    "compute_metrics(encoded_validation_labels, validation_predictions)\n",
    "\n",
    "print(\"Note: This evaluation is performed on the full validation set, not just tweets predicted as YES by previous models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "eabd0922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class IDEOLOGICAL-INEQUALITY: 1209 samples\n",
      "Class MISOGYNY-NON-SEXUAL-VIOLENCE: 868 samples\n",
      "Class OBJECTIFICATION: 1083 samples\n",
      "Class SEXUAL-VIOLENCE: 651 samples\n",
      "Class STEREOTYPING-DOMINANCE: 1337 samples\n"
     ]
    }
   ],
   "source": [
    "class_counts = encoded_training_labels.sum(axis=0)  # Sum over samples for each class\n",
    "\n",
    "for cls, count in zip(mlb.classes_, class_counts):\n",
    "    print(f\"Class {cls}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a39cfc",
   "metadata": {},
   "source": [
    "### Get previous YES statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f1a0415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids(ids):\n",
    "    with open(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/test/EXIST2025_test_clean.json\", \"r\") as f:\n",
    "        test_json = json.load(f)\n",
    "    # Filter to only English tweets in test set (optional sanity check)\n",
    "    tweets_all = [item for item in test_json.values() if item[\"lang\"] == \"en\"]\n",
    "\n",
    "    print(f\"Total English tweets in test set: {len(tweets_all)}\")\n",
    "\n",
    "    # Filter only English test tweets where id_EXIST is in yes_ids\n",
    "    yes_tweets = [\n",
    "        tweet_data[\"tweet\"]\n",
    "        for tweet_data in test_json.values()\n",
    "        if tweet_data[\"lang\"] == \"en\" and tweet_data[\"id_EXIST\"] in ids\n",
    "    ]\n",
    "\n",
    "    yes_en_ids = [\n",
    "        tweet_data[\"id_EXIST\"]\n",
    "        for tweet_data in test_json.values()\n",
    "        if tweet_data[\"lang\"] == \"en\" and tweet_data[\"id_EXIST\"] in ids\n",
    "    ]\n",
    "\n",
    "    print(f\"Number of English tweets predicted YES: {len(yes_tweets)}\")\n",
    "    print(yes_en_ids[:5])\n",
    "    print(yes_tweets[:5])\n",
    "    \n",
    "    return yes_en_ids, yes_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30da94a",
   "metadata": {},
   "source": [
    "### Bert (Best perfomance in tast 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c735d207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['600002', '600004', '600005', '600013', '600018']\n"
     ]
    }
   ],
   "source": [
    "# Load your BoW predictions from the JSON file\n",
    "with open(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/Task 1.1/Bert/bert_task1_submission.json\", \"r\") as f:\n",
    "    bert_results = json.load(f)\n",
    "\n",
    "# Extract the IDs that were predicted as \"YES\"\n",
    "yes_ids_bert = [entry[\"id\"] for entry in bert_results if entry[\"value\"] == \"YES\"]\n",
    "yes_ids_bert = [id_.replace(\"id_\", \"\") for id_ in yes_ids_bert]\n",
    "\n",
    "\n",
    "print(yes_ids_bert[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "44abfd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total English tweets in test set: 978\n",
      "Number of English tweets predicted YES: 460\n",
      "['600002', '600004', '600005', '600013', '600018']\n",
      "[\"@Cathy_TwoHalves @EverydaySexism That's why women have to stay at home, raise children and take care of the house.Nobody is going to assume that a woman is a leader among men since that's not how it is supposed to be in nature. Of course, you can try and fake it, but you will keep losing your feminine side.\", '@JonPincince @kmulvane @EverydaySexism, even after Roe v. Wade was overturned. Fun! /s', 'Seriously @JohnLewisRetail?  girls are pictured with ‘I’m a Stylist’ doll and the pizza oven - the boys get a robot and hot wheels garage. #EverydaySexism https://t.co/mKsbxaGgIV? # via @HPUKParents', '@tired_of_debate @ScoutSleepe @ImWatson91 The fact that some men assault transwomen too is awful &amp; unacceptable.Having said that—being harassed by men does NOT render MtFs “women” (+men *do* harass other men sometimes, too…)+It is notable that—for ALL sexual harassment/assault victims—the abuser is almost always MALE!', 'it’s not even 7pm and we are already being harassed by a drunk girl on the blue line and bonding with a fellow rider about it lmao']\n",
      "Tweet: @Cathy_TwoHalves @EverydaySexism That's why women have to stay at home, raise children and take care of the house.Nobody is going to assume that a woman is a leader among men since that's not how it is supposed to be in nature. Of course, you can try and fake it, but you will keep losing your feminine side.\n",
      "Predicted labels: ('IDEOLOGICAL-INEQUALITY', 'OBJECTIFICATION', 'STEREOTYPING-DOMINANCE')\n",
      "\n",
      "Tweet: @JonPincince @kmulvane @EverydaySexism, even after Roe v. Wade was overturned. Fun! /s\n",
      "Predicted labels: ('MISOGYNY-NON-SEXUAL-VIOLENCE',)\n",
      "\n",
      "Tweet: Seriously @JohnLewisRetail?  girls are pictured with ‘I’m a Stylist’ doll and the pizza oven - the boys get a robot and hot wheels garage. #EverydaySexism https://t.co/mKsbxaGgIV? # via @HPUKParents\n",
      "Predicted labels: ('OBJECTIFICATION', 'STEREOTYPING-DOMINANCE')\n",
      "\n",
      "Tweet: @tired_of_debate @ScoutSleepe @ImWatson91 The fact that some men assault transwomen too is awful &amp; unacceptable.Having said that—being harassed by men does NOT render MtFs “women” (+men *do* harass other men sometimes, too…)+It is notable that—for ALL sexual harassment/assault victims—the abuser is almost always MALE!\n",
      "Predicted labels: ('IDEOLOGICAL-INEQUALITY', 'MISOGYNY-NON-SEXUAL-VIOLENCE', 'SEXUAL-VIOLENCE', 'STEREOTYPING-DOMINANCE')\n",
      "\n",
      "Tweet: it’s not even 7pm and we are already being harassed by a drunk girl on the blue line and bonding with a fellow rider about it lmao\n",
      "Predicted labels: ('IDEOLOGICAL-INEQUALITY', 'MISOGYNY-NON-SEXUAL-VIOLENCE', 'STEREOTYPING-DOMINANCE')\n",
      "\n",
      "Saved 978 predictions to BoW_task3_submission.json (YES: 460, NO: 518)\n"
     ]
    }
   ],
   "source": [
    "# Assuming yes_ids_bert is a list of IDs to filter on\n",
    "filtered_ids, filtered_tweets = get_ids(yes_ids_bert)\n",
    "\n",
    "# Clean the filtered tweets\n",
    "cleaned_filtered_tweets = clean_text(filtered_tweets)\n",
    "\n",
    "# Transform tweets to TF-IDF features\n",
    "X_filtered_tfidf = tfidf_vectorizer.transform(cleaned_filtered_tweets)\n",
    "\n",
    "# Predict multi-label outputs (binary indicator arrays)\n",
    "filtered_predictions = logistic_regressor.predict(X_filtered_tfidf)\n",
    "\n",
    "# Convert binary predictions back to label sets (list of labels per tweet)\n",
    "filtered_pred_labels = mlb.inverse_transform(filtered_predictions)\n",
    "\n",
    "# Print first 5 tweets with their predicted labels\n",
    "for tweet, labels in zip(filtered_tweets[:5], filtered_pred_labels[:5]):\n",
    "    print(f\"Tweet: {tweet}\\nPredicted labels: {labels}\\n\")\n",
    "\n",
    "# Save predictions to JSON (adjusted for multi-label format)\n",
    "save_predictions_to_json(filtered_ids, filtered_pred_labels, \"BoW_task3_submission.json\", lang=\"en\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b9f30f",
   "metadata": {},
   "source": [
    "### Spanish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1698bbf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['IDEOLOGICAL-INEQUALITY' 'MISOGYNY-NON-SEXUAL-VIOLENCE' 'OBJECTIFICATION'\n",
      " 'SEXUAL-VIOLENCE' 'STEREOTYPING-DOMINANCE']\n",
      "Encoded shape: (2513, 5)\n"
     ]
    }
   ],
   "source": [
    "# === 2. Extract training and validation data ===\n",
    "training_tweets_raw = SpTrainTask3[1]      # Raw tweet texts from the training set\n",
    "training_labels_raw = SpTrainTask3[2]      # Corresponding labels for training tweets\n",
    "\n",
    "validation_tweets_raw = SpDevTask3[1]      # Raw tweet texts from the validation set\n",
    "validation_labels_raw = SpDevTask3[2]      # Corresponding labels for validation tweets\n",
    "\n",
    "# === 3. Clean the tweets (e.g., remove URLs, mentions, special characters) ===\n",
    "cleaned_training_tweets = clean_text(training_tweets_raw)\n",
    "cleaned_validation_tweets = clean_text(validation_tweets_raw)\n",
    "\n",
    "# Initialize MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Fit and transform on training labels\n",
    "encoded_training_labels = mlb.fit_transform(training_labels_raw)\n",
    "\n",
    "# Transform validation labels using the same mlb (no refitting)\n",
    "encoded_validation_labels = mlb.transform(validation_labels_raw)\n",
    "\n",
    "print(\"Classes:\", mlb.classes_)\n",
    "print(\"Encoded shape:\", encoded_training_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "acb69eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (subset accuracy): 0.115\n",
      "Macro F1: 0.665\n",
      "Macro Precision: 0.681\n",
      "Macro Recall: 0.651\n",
      "Singular ICM value (overall label accuracy): 0.506\n",
      "Note: This evaluation is performed on the full validation set, not just tweets predicted as YES by previous models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: divide by zero encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: overflow encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n",
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/linear_model/_linear_loss.py:209: RuntimeWarning: invalid value encountered in matmul\n",
      "  norm2_w = weights @ weights if weights.ndim == 1 else squared_norm(weights)\n"
     ]
    }
   ],
   "source": [
    "# === 4. Encode string labels as multi-label binarized vectors ===\n",
    "mlb = MultiLabelBinarizer()\n",
    "encoded_training_labels = mlb.fit_transform(training_labels_raw)\n",
    "encoded_validation_labels = mlb.transform(validation_labels_raw)\n",
    "\n",
    "# === 5. Convert cleaned text data into TF-IDF features ===\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))  # unigrams and bigrams\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(cleaned_training_tweets)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(cleaned_validation_tweets)\n",
    "\n",
    "# === 6. Train a One-vs-Rest Logistic Regression classifier for multi-label ===\n",
    "logistic_regressor = OneVsRestClassifier(LogisticRegression(max_iter=1000, class_weight='balanced'))\n",
    "logistic_regressor.fit(X_train_tfidf, encoded_training_labels)\n",
    "\n",
    "# === 7. Predict and evaluate on the validation set ===\n",
    "validation_predictions = logistic_regressor.predict(X_val_tfidf)\n",
    "\n",
    "# Calculate and display custom evaluation metrics\n",
    "compute_metrics(encoded_validation_labels, validation_predictions)\n",
    "\n",
    "print(\"Note: This evaluation is performed on the full validation set, not just tweets predicted as YES by previous models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "98491c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class IDEOLOGICAL-INEQUALITY: 1460 samples\n",
      "Class MISOGYNY-NON-SEXUAL-VIOLENCE: 1200 samples\n",
      "Class OBJECTIFICATION: 1359 samples\n",
      "Class SEXUAL-VIOLENCE: 820 samples\n",
      "Class STEREOTYPING-DOMINANCE: 1656 samples\n"
     ]
    }
   ],
   "source": [
    "class_counts = encoded_training_labels.sum(axis=0)  # Sum over samples for each class\n",
    "\n",
    "for cls, count in zip(mlb.classes_, class_counts):\n",
    "    print(f\"Class {cls}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b868c",
   "metadata": {},
   "source": [
    "### Bert (Best performance in task 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c1423b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids_spanish(ids):\n",
    "    with open(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/EXIST 2025 Tweets Dataset/test/EXIST2025_test_clean.json\", \"r\") as f:\n",
    "        test_json = json.load(f)\n",
    "    # Filter to only English tweets in test set (optional sanity check)\n",
    "    tweets_all = [item for item in test_json.values() if item[\"lang\"] == \"en\"]\n",
    "\n",
    "    print(f\"Total English tweets in test set: {len(tweets_all)}\")\n",
    "\n",
    "    # Filter only English test tweets where id_EXIST is in yes_ids\n",
    "    yes_tweets = [\n",
    "        tweet_data[\"tweet\"]\n",
    "        for tweet_data in test_json.values()\n",
    "        if tweet_data[\"lang\"] == \"es\" and tweet_data[\"id_EXIST\"] in ids\n",
    "    ]\n",
    "\n",
    "    yes_en_ids = [\n",
    "        tweet_data[\"id_EXIST\"]\n",
    "        for tweet_data in test_json.values()\n",
    "        if tweet_data[\"lang\"] == \"es\" and tweet_data[\"id_EXIST\"] in ids\n",
    "    ]\n",
    "\n",
    "    print(f\"Number of English tweets predicted YES: {len(yes_tweets)}\")\n",
    "    print(yes_en_ids[:5])\n",
    "    print(yes_tweets[:5])\n",
    "    \n",
    "    return yes_en_ids, yes_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a9cd0f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['500004', '500012', '500019', '500020', '500022']\n"
     ]
    }
   ],
   "source": [
    "# Load your BoW predictions from the JSON file\n",
    "with open(\"/Users/niklasdahlbom/Documents/Valencia/Lnr/Project/Task 1.1/Bert/bert_task1_submission_Spanish.json\", \"r\") as f:\n",
    "    bert_results = json.load(f)\n",
    "\n",
    "# Extract the IDs that were predicted as \"YES\"\n",
    "yes_ids_bert = [entry[\"id\"] for entry in bert_results if entry[\"value\"] == \"YES\"]\n",
    "yes_ids_bert = [id_.replace(\"id_\", \"\") for id_ in yes_ids_bert]\n",
    "\n",
    "\n",
    "print(yes_ids_bert[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "133a93ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total English tweets in test set: 978\n",
      "Number of English tweets predicted YES: 567\n",
      "['500004', '500012', '500019', '500020', '500022']\n",
      "['@jordirico Primero fue internet, luego el gamergate, la manosfera y su misoginia extrema sin que las plataformas movieran un dedo, los incel-asesinatos, la extrema derecha capitalizando el odio, la mimetización de estas ideas con las de un debate respetable y finalmente aquí estamos', '@dimplerrylover lo se pero literalmente la chica estaba siendo harassed like como puede sonreir asi', '@Fistroman1 @ElioGatsby Sin pruebas??Qué fue en público.Qué lo vimos todos, qué me estás contando.Que no me mezcles churras con merinas.Qué de aquí a que me vengas con el metoo y el notallmen queda poquito y no me apetece acabar discutiendo de verdad, q lo veo venir.', '@EstefaniaVeloz ...Con el proceso legal, no para hacer arder a una persona (No sé ni quienes sean en este caso). Hay mujeres que son víctimas pero también hay corruptas, violentas y victimarias. El movimiento #MeToo ya cumplió su cometido ¿Qué sigue? ¿Cómo avanzar sin que sea un arma a capricho?', '@geeksterilia Desde la Olivia que anda trepadota en ladrillo por ser directora.El video de la llama a Shia está así de la wey toda condescendiente para MI floQue para esto según ella le pidió a Shia retirarse de la pelo porque pues metooY Andale que nada. La wey fingiendo https://t.co/t9zCKx1IDs']\n",
      "Tweet: @jordirico Primero fue internet, luego el gamergate, la manosfera y su misoginia extrema sin que las plataformas movieran un dedo, los incel-asesinatos, la extrema derecha capitalizando el odio, la mimetización de estas ideas con las de un debate respetable y finalmente aquí estamos\n",
      "Predicted labels: ()\n",
      "\n",
      "Tweet: @dimplerrylover lo se pero literalmente la chica estaba siendo harassed like como puede sonreir asi\n",
      "Predicted labels: ('MISOGYNY-NON-SEXUAL-VIOLENCE', 'OBJECTIFICATION', 'SEXUAL-VIOLENCE', 'STEREOTYPING-DOMINANCE')\n",
      "\n",
      "Tweet: @Fistroman1 @ElioGatsby Sin pruebas??Qué fue en público.Qué lo vimos todos, qué me estás contando.Que no me mezcles churras con merinas.Qué de aquí a que me vengas con el metoo y el notallmen queda poquito y no me apetece acabar discutiendo de verdad, q lo veo venir.\n",
      "Predicted labels: ('OBJECTIFICATION',)\n",
      "\n",
      "Tweet: @EstefaniaVeloz ...Con el proceso legal, no para hacer arder a una persona (No sé ni quienes sean en este caso). Hay mujeres que son víctimas pero también hay corruptas, violentas y victimarias. El movimiento #MeToo ya cumplió su cometido ¿Qué sigue? ¿Cómo avanzar sin que sea un arma a capricho?\n",
      "Predicted labels: ('IDEOLOGICAL-INEQUALITY', 'MISOGYNY-NON-SEXUAL-VIOLENCE', 'OBJECTIFICATION', 'SEXUAL-VIOLENCE', 'STEREOTYPING-DOMINANCE')\n",
      "\n",
      "Tweet: @geeksterilia Desde la Olivia que anda trepadota en ladrillo por ser directora.El video de la llama a Shia está así de la wey toda condescendiente para MI floQue para esto según ella le pidió a Shia retirarse de la pelo porque pues metooY Andale que nada. La wey fingiendo https://t.co/t9zCKx1IDs\n",
      "Predicted labels: ()\n",
      "\n",
      "Saved 1098 predictions to BoW_task3_submission_Spanish.json (YES: 567, NO: 531)\n"
     ]
    }
   ],
   "source": [
    "# Assuming yes_ids_bert is a list of IDs to filter on\n",
    "filtered_ids, filtered_tweets = get_ids_spanish(yes_ids_bert)\n",
    "\n",
    "# Clean the filtered tweets\n",
    "cleaned_filtered_tweets = clean_text(filtered_tweets)\n",
    "\n",
    "# Transform tweets to TF-IDF features\n",
    "X_filtered_tfidf = tfidf_vectorizer.transform(cleaned_filtered_tweets)\n",
    "\n",
    "# Predict multi-label outputs (binary indicator arrays)\n",
    "filtered_predictions = logistic_regressor.predict(X_filtered_tfidf)\n",
    "\n",
    "# Convert binary predictions back to label sets (list of labels per tweet)\n",
    "filtered_pred_labels = mlb.inverse_transform(filtered_predictions)\n",
    "\n",
    "# Print first 5 tweets with their predicted labels\n",
    "for tweet, labels in zip(filtered_tweets[:5], filtered_pred_labels[:5]):\n",
    "    print(f\"Tweet: {tweet}\\nPredicted labels: {labels}\\n\")\n",
    "\n",
    "# Save predictions to JSON (adjusted for multi-label format)\n",
    "save_predictions_to_json(filtered_ids, filtered_pred_labels, \"BoW_task3_submission_Spanish.json\", lang=\"es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3648dece",
   "metadata": {},
   "source": [
    "### Merge files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6f3f61bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 978 EN + 1098 ES = 2076 total predictions.\n"
     ]
    }
   ],
   "source": [
    "filename_english = \"BoW_task3_submission.json\"\n",
    "filename_spanish = \"BoW_task3_submission_Spanish.json\"\n",
    "filename_merged = \"BoW_task3_submission_merge.json\"\n",
    "\n",
    "def merge_predictions(filename_english, filename_spanish, filename_merged):\n",
    "    # Load English predictions\n",
    "    with open(filename_english, \"r\", encoding=\"utf-8\") as f_en:\n",
    "        preds_en = json.load(f_en)\n",
    "\n",
    "    # Load Spanish predictions\n",
    "    with open(filename_spanish, \"r\", encoding=\"utf-8\") as f_es:\n",
    "        preds_es = json.load(f_es)\n",
    "\n",
    "    # Merge the two lists\n",
    "    merged_preds = preds_es + preds_en\n",
    "\n",
    "    # Save the combined predictions\n",
    "    with open(filename_merged, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(merged_preds, f_out, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Merged {len(preds_en)} EN + {len(preds_es)} ES = {len(merged_preds)} total predictions.\")\n",
    "    \n",
    "    \n",
    "merge_predictions(\n",
    "    filename_english,\n",
    "    filename_spanish,\n",
    "    filename_merged\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
